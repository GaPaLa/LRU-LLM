{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flax\n",
      "  Downloading flax-0.7.2-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.6/226.6 kB\u001b[0m \u001b[31m948.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorstore\n",
      "  Downloading tensorstore-0.1.43-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting orbax-checkpoint\n",
      "  Downloading orbax_checkpoint-0.2.3-py3-none-any.whl (81 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (1.23.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (6.0)\n",
      "Collecting rich>=11.1\n",
      "  Downloading rich-13.5.3-py3-none-any.whl (239 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.8/239.8 kB\u001b[0m \u001b[31m984.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting msgpack\n",
      "  Downloading msgpack-1.0.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (322 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.1.1 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (4.5.0)\n",
      "Collecting optax\n",
      "  Downloading optax-0.1.7-py3-none-any.whl (154 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jax>=0.4.2 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.4.13)\n",
      "Requirement already satisfied: scipy>=1.7 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (1.9.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (6.6.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (3.3.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.8/site-packages (from rich>=11.1->flax) (2.16.1)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jaxlib>=0.1.37 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from optax->flax) (0.4.13+cuda12.cudnn89)\n",
      "Collecting absl-py>=0.7.1\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting chex>=0.1.5\n",
      "  Downloading chex-0.1.7-py3-none-any.whl (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.6/89.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nest_asyncio in ./.local/lib/python3.8/site-packages (from orbax-checkpoint->flax) (1.5.8)\n",
      "Requirement already satisfied: importlib_resources in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from orbax-checkpoint->flax) (5.12.0)\n",
      "Collecting etils\n",
      "  Downloading etils-1.3.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cached_property\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting toolz>=0.9.0\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting dm-tree>=0.1.5\n",
      "  Downloading dm_tree-0.1.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /media/user/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.6->jax>=0.4.2->flax) (3.15.0)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: msgpack, dm-tree, cached_property, toolz, tensorstore, mdurl, etils, absl-py, markdown-it-py, rich, orbax-checkpoint, chex, optax, flax\n",
      "Successfully installed absl-py-1.4.0 cached_property-1.5.2 chex-0.1.7 dm-tree-0.1.8 etils-1.3.0 flax-0.7.2 markdown-it-py-3.0.0 mdurl-0.1.2 msgpack-1.0.5 optax-0.1.7 orbax-checkpoint-0.2.3 rich-13.5.3 tensorstore-0.1.43 toolz-0.12.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paper https://arxiv.org/abs/2303.06349 cals the code they provide a \"simplified\" implmementation - what did they leave out?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Here is the output without line numbers:\n",
    "\n",
    "#the help of Lemma 3.2, with phase potentially restricted to a thin slice (see §3.4).\n",
    "import jax\n",
    "from jax import random\n",
    "from flax import linen as nn\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from flax.linen import initializers\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "from flax.training import train_state\n",
    "from flax.training.common_utils import shard, get_metrics\n",
    "parallel_scan = jax.lax.associative_scan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    \"\"\"Linear Recurrent Unit (LRU) layer\"\"\"\n",
    "    state_dim:int\n",
    "    embed_dim:int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "\n",
    "    def setup(self):\n",
    "        self.B_re = self.param('B_re', initializers.glorot_normal(), (self.state_dim, self.embed_dim))\n",
    "        self.B_im = self.param('B_im', initializers.glorot_normal(), (self.state_dim, self.embed_dim))\n",
    "        self.C_re = self.param('C_re', initializers.glorot_normal(), (self.embed_dim, self.state_dim))\n",
    "        self.C_im = self.param('C_im', initializers.glorot_normal(), (self.embed_dim, self.state_dim))\n",
    "        self.D = self.param('D', initializers.normal(), (self.embed_dim,))\n",
    "        \n",
    "        u1 = np.random.uniform(size=(self.state_dim,))\n",
    "        u2 = np.random.uniform(size=(self.state_dim,))\n",
    "        nu_log = np.log(-0.5*np.log(u1*(self.r_max**2-self.r_min**2) + self.r_min**2))\n",
    "        theta_log = np.log(self.max_phase*u2)\n",
    "        \n",
    "        diag_lambda = np.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "        gamma_log = np.log(jnp.sqrt(1-jnp.abs(diag_lambda)**2))\n",
    "\n",
    "        # Initialize the parameters here\n",
    "        self.nu_log = self.param('nu_log', lambda rng, shape: nu_log, ())\n",
    "        self.theta_log = self.param('theta_log', lambda rng, shape: theta_log, ())\n",
    "        self.gamma_log = self.param('gamma_log', lambda rng, shape: gamma_log, ())\n",
    "\n",
    "    def __call__(self, input_sequence):\n",
    "        \"\"\"Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).\"\"\"\n",
    "\n",
    "        # Materializing the diagonal of Lambda and projections\n",
    "        Lambda = jnp.exp(-jnp.exp(self.nu_log) + 1j*jnp.exp(self.theta_log))\n",
    "        B_norm = (self.B_re + 1j*self.B_im) * jnp.expand_dims(jnp.exp(self.gamma_log), axis=-1)\n",
    "        C = self.C_re + 1j*self.C_im\n",
    "        \n",
    "        # Running the LRU + output projection\n",
    "        # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "        if True:\n",
    "            Lambda_elements = jnp.repeat(Lambda[None, None, :], input_sequence.shape[0], axis=0)\n",
    "            Lambda_elements = jnp.repeat(Lambda_elements, input_sequence.shape[1], axis=1)\n",
    "\n",
    "            Bu_elements = jax.vmap(jax.vmap(lambda u: B_norm @ u))(input_sequence)\n",
    "\n",
    "            \n",
    "\n",
    "        else:\n",
    "            Lambda_elements = jnp.repeat(Lambda[None, ...], input_sequence.shape[0], axis=0)\n",
    "            print('lamba elements',Lambda_elements.shape)\n",
    "\n",
    "            Bu_elements = jax.vmap(lambda u: B_norm @ u)(input_sequence)\n",
    "            print('Bu_elements',Bu_elements.shape)\n",
    "\n",
    "        elements = (Lambda_elements, Bu_elements)\n",
    "        _, inner_states = parallel_scan(self.binary_operator_diag, elements, axis=1) # all x_k\n",
    "        #print(Bu_elements)\n",
    "        #print(inner_states.mean(axis=-1))\n",
    "        #y = jax.vmap(jax.vmap(lambda x, u: (C @ x).real + self.D * u))(inner_states, input_sequence)\n",
    "        y = jax.vmap(jax.vmap(lambda x, u: (C @ x).real + self.D * u))(inner_states, input_sequence)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def binary_operator_diag(self, element_i, element_j):\n",
    "\n",
    "        # Binary operator for parallel scan of linear recurrence.\n",
    "        a_i, bu_i = element_i\n",
    "        a_j, bu_j = element_j\n",
    "\n",
    "        return a_j * a_i, a_j * bu_i + bu_j\n",
    "    \n",
    "\n",
    "\n",
    "# generate random vector\n",
    "key1 = random.PRNGKey(0)\n",
    "x1 = jnp.asarray([[[0.1,0.5], [0.2,0.2], [0.3,0.3]],[[0.9,0.9], [0.8,0.8], [0.7,0.7]]]) #jax.random.normal(key1, shape=(1,3,64),)\n",
    "x2 = jnp.asarray([[[0.1,0.1], [0.2,0.2], [0.3,0.3]],[[0.9,0.9], [0.8,0.8], [0.7,0.7]]]) #jax.random.normal(key1, shape=(1,3,64),)\n",
    "\n",
    "key2 = jax.random.PRNGKey(0)  # Set a random key for reproducibility\n",
    "lru = LRU(embed_dim=2, state_dim=2)\n",
    "params = lru.init(key2, x1)\n",
    "\n",
    "print('\\n\\n')\n",
    "y = lru.apply(params, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 2)\n",
      "[[[  0.26570448  -0.12417762]\n",
      "  [  9.270536    -3.246264  ]\n",
      "  [ 26.527102    -0.87200737]]\n",
      "\n",
      " [[ 34.693897   -11.8206415 ]\n",
      "  [ 30.840815   -10.508144  ]\n",
      "  [ 26.988638    -9.196105  ]]]\n",
      "[[[  3.7415366  -1.3212094]\n",
      "  [  7.7269716  -2.6466095]\n",
      "  [ 11.5918255  -3.9552112]]\n",
      "\n",
      " [[ 34.693897  -11.8206415]\n",
      "  [ 30.840815  -10.508144 ]\n",
      "  [ 26.988638   -9.196105 ]]]\n"
     ]
    }
   ],
   "source": [
    "class FFW(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.up = nn.Dense(self.FFW_dim)\n",
    "        self.down = nn.Dense(self.embed_dim)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.up(x)\n",
    "        x = nn.activation.silu(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# generate random vector\n",
    "key1 = random.PRNGKey(0)\n",
    "x1 = jnp.asarray([[[10,0], [20,25], [2,37]],[[90,90], [80,80], [70,70]]]) #jax.random.normal(key1, shape=(1,3,64),)\n",
    "x2 = jnp.asarray([[[10,10], [20,20], [30,30]],[[90,90], [80,80], [70,70]]]) #jax.random.normal(key1, shape=(1,3,64),)\n",
    "\n",
    "key2 = jax.random.PRNGKey(0)  # Set a random key for reproducibility\n",
    "lru_block = FFW(embed_dim=2, FFW_dim=8)\n",
    "params = lru_block.init(key2, x1)\n",
    "print(x1.shape)\n",
    "y = lru_block.apply(params, x1)\n",
    "print(y)\n",
    "\n",
    "y = lru_block.apply(params, x2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_block(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "\n",
    "    def setup(self):\n",
    "        self.ffw = FFW(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim)\n",
    "        self.lru = LRU(embed_dim=self.embed_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase)\n",
    "        self.norm1 = nn.LayerNorm()\n",
    "        self.norm2 = nn.LayerNorm()\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.lru(x)\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# generate random vector\n",
    "key1 = random.PRNGKey(0)\n",
    "x1 = jnp.asarray([[[10,0], [20,20], [30,30]],[[90,90], [80,80], [70,70]]]) #jax.random.normal(key1, shape=(1,3,64),)\n",
    "x2 = jnp.asarray([[[10,10], [20,20], [30,30]],[[90,90], [80,80], [70,70]]]) #jax.random.normal(key1, shape=(1,3,64),)\n",
    "\n",
    "key2 = jax.random.PRNGKey(0)  # Set a random key for reproducibility\n",
    "lru_block = LRU_block(embed_dim=2, state_dim=8, FFW_dim=8)\n",
    "params = lru_block.init(key2, x1)\n",
    "\n",
    "y = lru_block.apply(params, x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_LLM(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    layers: int    \n",
    "    vocab_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Dense(self.embed_dim)\n",
    "        self.blocks = [LRU_block(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, state_dim=self.state_dim)]\n",
    "        self.final_norm = nn.LayerNorm()\n",
    "        self.unembed = nn.Dense(self.vocab_size)\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        x = jax.nn.one_hot(x, num_classes=self.vocab_size)\n",
    "\n",
    "        # embed tokens\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # pass through all LRU blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # final ln\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        # Get transposed weights\n",
    "        #weights = jnp.transpose(self.embed['params']['embedding']) \n",
    "        #unembed = flax.linen.Linear(weights=weights)\n",
    "                \n",
    "        # softmax projection\n",
    "        return self.unembed(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607283\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "vocab = list(set(open('/media/user/Z/PythonQA.txt').read()))\n",
    "vocab.insert(0, '</s>')\n",
    "vocab.insert(0, '<s>')\n",
    "vocab.insert(0, '<unk>')\n",
    "\n",
    "dataset_samples = open('/media/user/Z/PythonQA.txt').read().split('<|endoftext|>')\n",
    "\n",
    "print(len(dataset_samples))\n",
    "char2idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx2char = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "vocab_size = 256 #len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_size = 512\n",
    "embed_dim = 256\n",
    "FFW_dim = 512\n",
    "state_dim = 128\n",
    "layers = 2\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "# generate random vector\n",
    "key1 = random.PRNGKey(0)\n",
    "x = jax.random.randint(key1, shape=(8,512), minval=0, maxval=10, dtype=jnp.uint32)\n",
    "\n",
    "key2 = jax.random.PRNGKey(0)  # Set a random key for reproducibility\n",
    "lru_LLM = LRU_LLM(embed_dim=embed_dim, FFW_dim=FFW_dim, state_dim=state_dim, layers=layers, vocab_size=vocab_size)\n",
    "params = lru_LLM.init(key2, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character tokenizer\n",
    "\n",
    "class SimpleDataLoader:\n",
    "    def __init__(self, dataset_samples, char2idx, batch_size):\n",
    "        self.dataset_samples = dataset_samples\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = len(char2idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_samples)\n",
    "\n",
    "    def get_batch(self, index):\n",
    "        batch_samples = self.dataset_samples[index : index + self.batch_size]\n",
    "        batch_input = []\n",
    "        batch_target = []\n",
    "        for sample in batch_samples:\n",
    "            input_ids = self.tokenize(sample)\n",
    "            batch_input.append(input_ids[1:])\n",
    "            batch_target.append(input_ids[:-1])\n",
    "        return jnp.array(batch_input), jnp.array(batch_target)\n",
    "\n",
    "    def tokenize(self, text, max_length):\n",
    "        list = [char2idx[ch] for ch in text]\n",
    "        list.insert(0,1)\n",
    "        list.append(2)\n",
    "        list += [2 for i in range(max_length-len(list))]\n",
    "        list = list[:max_length]\n",
    "        return list\n",
    "    \n",
    "    def detokenize(self, ids):\n",
    "        return \"\".join([self.idx2char[i] for i in ids])\n",
    "\n",
    "data_loader = SimpleDataLoader(dataset_samples, char2idx, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 256)\n",
      "(1, 3, 256)\n",
      "(1, 4, 256)\n",
      "(1, 5, 256)\n",
      "(1, 6, 256)\n",
      "(1, 7, 256)\n",
      "(1, 8, 256)\n",
      "(1, 9, 256)\n",
      "(1, 10, 256)\n",
      "(1, 11, 256)\n",
      "(1, 12, 256)\n",
      "(1, 13, 256)\n",
      "(1, 14, 256)\n",
      "(1, 15, 256)\n",
      "(1, 16, 256)\n",
      "(1, 17, 256)\n",
      "(1, 18, 256)\n",
      "(1, 19, 256)\n",
      "(1, 20, 256)\n",
      "(1, 21, 256)\n",
      "(1, 22, 256)\n",
      "(1, 23, 256)\n",
      "(1, 24, 256)\n",
      "(1, 25, 256)\n",
      "(1, 26, 256)\n",
      "(1, 27, 256)\n",
      "(1, 28, 256)\n",
      "(1, 29, 256)\n",
      "(1, 30, 256)\n",
      "(1, 31, 256)\n",
      "[ 0.04205338 -0.00910356  0.03542134  0.10676627  0.05018805  0.03164284\n",
      "  0.02245259  0.00570123  0.08475644 -0.0323355   0.0337486  -0.02981921\n",
      " -0.13637358 -0.06288107  0.03588936  0.05091178 -0.0609285  -0.09887794\n",
      "  0.02181756 -0.00181809 -0.05359057  0.09079576  0.11184372  0.00979271\n",
      " -0.03580133  0.00402478 -0.00833572 -0.07054727 -0.03173715  0.03292423\n",
      "  0.07681849]\n",
      "hi9Ò=±/$Y¤^¡ÃÕRîÆ®.\n",
      "»[DÕ8Z!¢ÌîÌ\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "\n",
    "def generate(model, seed, gen_length):\n",
    "    \"\"\"\n",
    "    Generates text using the LRU_LLM language model.\n",
    "    \n",
    "    Args:\n",
    "        model: LRU_LLM model instance.\n",
    "        seed: Initial seed text to start generation.\n",
    "        max_length: Maximum length of the generated text.\n",
    "    \n",
    "    Returns:\n",
    "        Generated text as a string.\n",
    "    \"\"\"\n",
    "    current_length = len(seed)\n",
    "    generated_text = seed\n",
    "\n",
    "    while current_length < gen_length:\n",
    "        # Convert the generated text to token IDs\n",
    "        input_ids = jnp.array([data_loader.tokenize(generated_text, current_length)])\n",
    "\n",
    "        # Perform a forward pass through the model\n",
    "        logits = model.apply(params, input_ids)\n",
    "        print(logits.shape)\n",
    "\n",
    "        # Sample the next token from the logits\n",
    "        #next_token_id = jax.random.categorical(logits=logits[0,-1], key=key2).item()\n",
    "        next_token_id = np.argmax(logits[0,-1][0:217]).item()\n",
    "\n",
    "        # Convert the next token ID to a character\n",
    "        next_token = data_loader.detokenize([next_token_id])\n",
    "\n",
    "        # Append the next token to the generated text\n",
    "        generated_text += next_token\n",
    "\n",
    "        current_length += 1\n",
    "\n",
    "    print(logits[0].mean(axis=-1))\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "print(generate(lru_LLM, 'hi', gen_length=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04577552 -0.01078083  0.08290634 -0.01238275  0.14061648 -0.06481279\n",
      "  -0.09183305  0.01484728  0.051723   -0.02798086]\n",
      " [-0.03113073  0.03057463  0.04242035  0.11461998 -0.03873058  0.01570053\n",
      "  -0.03451788  0.05533995  0.02322705  0.12438513]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = jnp.asarray([[1,2,3,4,5,6,7,8,9,10],[11,12,13,14,15,12,17,18,19,20]])\n",
    "logits = lru_LLM.apply(params, input_ids)\n",
    "print(logits.mean(axis=(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04577552 -0.01078083  0.08290634 -0.01238275  0.14061648 -0.06481279\n",
      "  -0.09183305  0.01484728  0.051723   -0.02798086]\n",
      " [ 0.04038882  0.03057463  0.04242035  0.11461998 -0.03873058  0.01570053\n",
      "  -0.03451788  0.05533995  0.02322705  0.12438513]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = jnp.asarray([[1,2,3,4,5,6,7,8,9,10],[10003,12,13,14,15,12,17,18,19,20]])\n",
    "logits = lru_LLM.apply(params, input_ids)\n",
    "print(logits.mean(axis=(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0475834  -0.01078083  0.08290634 -0.01238275  0.14061648 -0.06481279\n",
      "  -0.09183305  0.01484728  0.051723   -0.02798086]\n",
      " [-0.082854    0.03057463  0.04242035  0.11461998 -0.03873058  0.01570053\n",
      "  -0.03451788  0.05533995  0.02322705  0.12438513]]\n"
     ]
    }
   ],
   "source": [
    "input_ids = jnp.asarray([[100,2,3,4,5,6,7,8,9,10],[11,12,13,14,15,12,17,18,19,20]])\n",
    "logits = lru_LLM.apply(params, input_ids)\n",
    "print(logits.mean(axis=(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CHECKPOINT_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 8\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X24sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m CHECKPOINT_PATH\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mTrainerModule\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, model_name, exmp_batch, max_iters, lr\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, warmup\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CHECKPOINT_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Model and optimizer\n",
    "tx = optax.adam(learning_rate=1e-3)\n",
    "state = train_state.TrainState.create(apply_fn=lru_LLM.apply, params=lru_LLM.init(jax.random.PRNGKey(0), jnp.ones([1, ctx_size])), tx=tx)\n",
    "\n",
    "train_steps = len(data_loader) // batch_size  # Number of steps per epoch\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):\n",
    "\n",
    "    # Linear warmup and cosine decay\n",
    "    lr = tx.init_fn(state.step)\n",
    "    lr = lr * jnp.minimum(1., state.step/1000) * 0.5 * (1. + jnp.cos(jnp.pi * state.step/10000))\n",
    "    state = tx.update_fn(lr, state.optim_state, state.params)\n",
    "\n",
    "    for step in range(train_steps):\n",
    "        xs, ys = data_loader.get_batch(step * batch_size)\n",
    "\n",
    "        loss = jnp.mean(nn.softmax_cross_entropy(lru_LLM(xs), ys))\n",
    "        state = state.apply_gradients(grads=jax.grad(loss)(state.params))\n",
    "\n",
    "# Generate text\n",
    "print(detokenize(lru_LLM.generate(temperature=1.0, length=1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard language model parameters:\n",
    "# context length (implicit)\n",
    "# hidden dim, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lru.apply(params, x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
