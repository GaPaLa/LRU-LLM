{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Requirement already satisfied: jax[cuda12_pip] in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (0.4.13)\n",
      "Requirement already satisfied: scipy>=1.7 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (1.9.3)\n",
      "Requirement already satisfied: opt-einsum in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (3.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (6.6.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (1.23.4)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (0.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvcc-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.2.140)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.2.140)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12>=8.9 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (8.9.4.25)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.2.142)\n",
      "Requirement already satisfied: jaxlib==0.4.13+cuda12.cudnn89 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (0.4.13+cuda12.cudnn89)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.1.2.141)\n",
      "Requirement already satisfied: nvidia-cublas-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.2.5.6)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (11.5.2.141)\n",
      "Requirement already satisfied: nvidia-cufft-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (11.0.8.103)\n",
      "Requirement already satisfied: zipp>=0.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.6->jax[cuda12_pip]) (3.15.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from nvidia-cudnn-cu12>=8.9->jax[cuda12_pip]) (12.2.140)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from nvidia-cusolver-cu12->jax[cuda12_pip]) (12.2.140)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: flax in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (0.7.2)\n",
      "Requirement already satisfied: tensorstore in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.1.43)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (6.0)\n",
      "Requirement already satisfied: jax>=0.4.2 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.4.13)\n",
      "Requirement already satisfied: orbax-checkpoint in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.2.3)\n",
      "Requirement already satisfied: rich>=11.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (13.5.3)\n",
      "Requirement already satisfied: numpy>=1.12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (1.23.4)\n",
      "Requirement already satisfied: msgpack in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (4.5.0)\n",
      "Requirement already satisfied: optax in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.1.7)\n",
      "Requirement already satisfied: opt-einsum in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (1.9.3)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (6.6.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.8/site-packages (from rich>=11.1->flax) (2.16.1)\n",
      "Requirement already satisfied: chex>=0.1.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from optax->flax) (0.1.7)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from optax->flax) (0.4.13+cuda12.cudnn89)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from optax->flax) (1.4.0)\n",
      "Requirement already satisfied: etils in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from orbax-checkpoint->flax) (1.3.0)\n",
      "Requirement already satisfied: importlib_resources in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from orbax-checkpoint->flax) (5.12.0)\n",
      "Requirement already satisfied: nest_asyncio in ./.local/lib/python3.8/site-packages (from orbax-checkpoint->flax) (1.5.8)\n",
      "Requirement already satisfied: cached_property in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from orbax-checkpoint->flax) (1.5.2)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from chex>=0.1.5->optax->flax) (0.1.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.6->jax>=0.4.2->flax) (3.15.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install flax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.linen import initializers\n",
    "import numpy as np\n",
    "from flax.training.common_utils import shard, get_metrics\n",
    "import optax\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRU layer definition\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    \"\"\"Linear Recurrent Unit (LRU) layer\"\"\"\n",
    "    state_dim:int\n",
    "    embed_dim:int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        # weights\n",
    "        self.B_re = self.param('B_re', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.B_im = self.param('B_im', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.C_re = self.param('C_re', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.C_im = self.param('C_im', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.D = self.param('D', initializers.normal(dtype=self.dtype), (self.embed_dim,))\n",
    "        \n",
    "        u1 = np.random.uniform(size=(self.state_dim,))\n",
    "        u2 = np.random.uniform(size=(self.state_dim,))\n",
    "        nu_log = np.log(-0.5*np.log(u1*(self.r_max**2-self.r_min**2) + self.r_min**2))\n",
    "        theta_log = np.log(self.max_phase*u2).astype(self.dtype)\n",
    "        \n",
    "        diag_lambda = np.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "        gamma_log = np.log(jnp.sqrt(1-jnp.abs(diag_lambda)**2))\n",
    "\n",
    "        # Initialize the parameters here\n",
    "        self.nu_log = self.param('nu_log', lambda rng, shape: nu_log, ())\n",
    "        self.theta_log = self.param('theta_log', lambda rng, shape: theta_log, ())\n",
    "        self.gamma_log = self.param('gamma_log', lambda rng, shape: gamma_log, ())\n",
    "\n",
    "    def __call__(self, input_sequence):\n",
    "        \"\"\"Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).\"\"\"\n",
    "\n",
    "        # Materializing the diagonal of Lambda and projections\n",
    "        Lambda = jnp.exp(-jnp.exp(self.nu_log) + 1j*jnp.exp(self.theta_log))\n",
    "        B_norm = (self.B_re + 1j*self.B_im) * jnp.expand_dims(jnp.exp(self.gamma_log), axis=-1)\n",
    "        C = self.C_re + 1j*self.C_im\n",
    "        \n",
    "        # Running the LRU + output projection\n",
    "        # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "        Lambda_elements = jnp.repeat(Lambda[None, None, :], input_sequence.shape[0], axis=0)\n",
    "        Lambda_elements = jnp.repeat(Lambda_elements, input_sequence.shape[1], axis=1)\n",
    "\n",
    "        Bu_elements = jax.vmap(jax.vmap(lambda u: B_norm @ u))(input_sequence)\n",
    "\n",
    "        elements = (Lambda_elements, Bu_elements)\n",
    "        _, inner_states = parallel_scan(self.binary_operator_diag, elements, axis=1) # all x_k\n",
    "        y = jax.vmap(jax.vmap(lambda x, u: (C @ x).real + self.D * u))(inner_states, input_sequence)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def binary_operator_diag(self, element_i, element_j):\n",
    "\n",
    "        # Binary operator for parallel scan of linear recurrence.\n",
    "        a_i, bu_i = element_i\n",
    "        a_j, bu_j = element_j\n",
    "\n",
    "        return a_j * a_i, a_j * bu_i + bu_j\n",
    "\n",
    "\n",
    "if False:\n",
    "    embed_dim = 256\n",
    "    lru_state_dim = 192\n",
    "    key1 = random.PRNGKey(0) # generate random vector for reproducability\n",
    "    x = jnp.ones(shape=(1,256,embed_dim), dtype=jnp.float32)\n",
    "    lru_LLM = LRU(embed_dim=embed_dim, state_dim=lru_state_dim, r_min=0.5, r_max=0.9, max_phase=2*math.pi, dtype=jnp.bfloat16) # LRU hyperparameters from LRU paper\n",
    "    lru_LLM_params = lru_LLM.init(key1, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFW(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.up = nn.Dense(self.FFW_dim, use_bias=False, dtype=self.dtype)\n",
    "        self.down = nn.Dense(self.embed_dim, use_bias=False, dtype=self.dtype)    \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        x = self.up(x)\n",
    "        x = nn.activation.silu(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_block(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.ffw = FFW(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, dtype=self.dtype)\n",
    "        self.lru = LRU(embed_dim=self.embed_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype)\n",
    "        self.norm1 = nn.RMSNorm(dtype=self.dtype)\n",
    "        self.norm2 = nn.RMSNorm(dtype=self.dtype)\n",
    "\n",
    "    def __call__(self, x): # preln\n",
    "        x = x + self.lru(self.norm1(x))\n",
    "        \n",
    "        x = x + self.ffw(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_LLM(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    layers: int    \n",
    "    vocab_size: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "    tie_weights: bool = True\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(features=self.embed_dim, num_embeddings=self.vocab_size, dtype=self.dtype)\n",
    "        self.blocks = [LRU_block(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype) for _ in range(self.layers)]\n",
    "        self.final_norm = nn.LayerNorm(dtype=self.dtype)\n",
    "\n",
    "        \n",
    "        if not self.tie_weights:\n",
    "            self.unembed = nn.Dense(self.vocab_size, dtype=self.dtype) # if not weight tied\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # x = jax.nn.one_hot(x, num_classes=self.vocab_size, dtype=self.dtype)\n",
    "\n",
    "        # embed tokens\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # pass through all LRU blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # final ln\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if self.tie_weights:\n",
    "            logits = self.embed.attend(x)\n",
    "        else:\n",
    "            logits = self.unembed(x)\n",
    "\n",
    "        # softmax projection\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- autoregressive generative inference\n",
    "from transformers import top_k_top_p_filtering\n",
    "\n",
    "def predictions(tokens, model, params, wanted_index=-1, temp=0.3):\n",
    "    \n",
    "    # Perform a forward pass through the model\n",
    "    input_ids = jnp.array([tokens])\n",
    "    logits = model.apply(params, input_ids)\n",
    "\n",
    "    # greedy decoding\n",
    "    if False:\n",
    "        # Sample the next token from the logits\n",
    "        #next_token_id = jax.random.categorical(logits=logits[0,-1][0:217], key=random.PRNGKey(0)).item()\n",
    "        tokens = np.argmax(logits[0,:,0:vocab_size],axis=-1) # get most recent token\n",
    "        print(tokens)\n",
    "        return tokens\n",
    "\n",
    "    # nucleus sampling\n",
    "    else:\n",
    "        logits = torch.tensor(np.asarray(logits.astype(jnp.float32)))[:,wanted_index,0:vocab_size]\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_p=temp)\n",
    "        probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "        predicted_token = torch.multinomial(probabilities, 1)\n",
    "        return predicted_token\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# instead of iteratively predicting the next token then concatenating\n",
    "# it to contect then predicting for contiually increasing sizes (which induces compilation for each new input size, which is very slow)\n",
    "# just extend the context to the final target generation size, padding with </s>\n",
    "def generate(model, params, gen_length, prompt='', temp=0.2): \n",
    "    \"\"\"\n",
    "    Generates text using the LRU_LLM language model.\n",
    "    \n",
    "    Args:\n",
    "        model: LRU_LLM model instance.\n",
    "        seed: Initial seed text to start generation.\n",
    "        max_length: Maximum length of the generated text.\n",
    "    \n",
    "    Returns:\n",
    "        Generated text as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize input text    \n",
    "    generated_text = prompt\n",
    "    tokens = tokenizer.tokenize(generated_text, max_length=gen_length)\n",
    "    padding_free = tokenizer.tokenize(generated_text) # generate without padding\n",
    "    tokens_length = len(padding_free) - 1 # + 1 for BOS, -1 for EOS\n",
    "\n",
    "    # get next token prediction for all tokens, including padding.\n",
    "    # Set the first </s> in context to the predicted next token, then iterate.\n",
    "    for i in range(tokens_length, gen_length):\n",
    "        to_ = i\n",
    "        from_ = i-1\n",
    "        #tokens[i] = predictions(tokens, model, params)[i-1].item()\n",
    "        predicted_token = predictions(tokens, model, params, wanted_index=from_, temp=temp).item()\n",
    "        tokens[to_] = predicted_token\n",
    "\n",
    "    generated_text = tokenizer.detokenize(tokens)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.14 in ./.local/lib/python3.8/site-packages (0.14.0)\n",
      "Collecting huggingface_hub<0.17,>=0.16.4\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (6.0)\n",
      "Requirement already satisfied: fsspec in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (2023.5.0)\n",
      "Requirement already satisfied: filelock in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (3.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (23.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (2.31.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (2022.12.7)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "Successfully installed huggingface_hub-0.16.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (0.16.4)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: filelock in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub) (1.26.12)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.16.4\n",
      "    Uninstalling huggingface-hub-0.16.4:\n",
      "      Successfully uninstalled huggingface-hub-0.16.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tokenizers 0.14.0 requires huggingface_hub<0.17,>=0.16.4, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.17.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "path_to_dataset_txt = '/media/idmi/Z/PythonQA.txt'\n",
    "path_to_dataset_txt = '/media/idmi/Z/tinystories.txt'\n",
    "\n",
    "dataset_samples = open(path_to_dataset_txt).read().split('<|endoftext|>')\n",
    "\n",
    "if False:\n",
    "    chars = list(set(open(path_to_dataset_txt).read()))\n",
    "    chars.insert(0, '</s>')\n",
    "    chars.insert(0, '<s>')\n",
    "    chars.insert(0, '<unk>')\n",
    "\n",
    "\n",
    "    char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Tokenizer and Data loading\n",
    "import torch\n",
    "\n",
    "# --- pretrained subword tokenizer from Llama2\n",
    "class Llama2_Tokenizer():\n",
    "    !pip install tokenizers==0.14\n",
    "    !pip install -U huggingface_hub\n",
    "    from transformers import AutoTokenizer\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    except:\n",
    "        login()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    vocab_size =  32000\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        llamaencoded = self.tokenizer.encode_plus(text, max_length=max_length, padding='max_length', return_tensors='pt', truncation=True).input_ids[0].tolist()\n",
    "        if max_length is None:\n",
    "            llamaencoded.append(2)\n",
    "\n",
    "        return llamaencoded\n",
    "    \n",
    "\n",
    "    def detokenize(self, text):\n",
    "        return self.tokenizer.decode(torch.tensor(text))\n",
    "\n",
    "# --- character-level tokenizer\n",
    "class Char_Tokenizer():\n",
    "    def __init__(self):\n",
    "        global vocab_size\n",
    "        vocab_size = len(chars)\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        list = [char2idx[ch] for ch in text]\n",
    "        list.insert(0,1)\n",
    "        list.append(2)\n",
    "        if max_length is None:\n",
    "            return list\n",
    "        else: # padding/cropping if max length is specified\n",
    "            list += [2 for i in range(max(0,max_length-len(list)))]\n",
    "            list = list[0:max_length]\n",
    "            return list\n",
    "    \n",
    "    def detokenize(self, ids):\n",
    "        return \"\".join([self.idx2char[i] for i in ids])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Data loader\n",
    "class SimpleDataLoader:\n",
    "    def __init__(self, dataset_samples, batch_size, context_length, tokenizer):\n",
    "        self.context_length=context_length\n",
    "        self.dataset_samples = dataset_samples\n",
    "        if False:\n",
    "            self.char2idx = char2idx\n",
    "            self.idx2char = idx2char\n",
    "            self.vocab_size = len(char2idx)\n",
    "        else:\n",
    "            vocab_size = tokenizer.vocab_size\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_samples)\n",
    "\n",
    "    def get_batch(self, index):\n",
    "        batch_samples = self.dataset_samples[index : index + self.batch_size]\n",
    "        batch_input = []\n",
    "        batch_target = []\n",
    "        for sample in batch_samples:\n",
    "            input_ids = self.tokenizer.tokenize(sample, max_length=self.context_length)\n",
    "            batch_input.append(input_ids[:-1]) # BOS,1,2,3,4,...\n",
    "            batch_target.append(input_ids[1:]) # 1,2,3,4,...EOS\n",
    "        return jnp.array(batch_input), jnp.array(batch_target)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 22:21:41.296298: W external/xla/xla/service/gpu/nvptx_compiler.cc:596] The NVIDIA driver's CUDA version is 12.1 which is older than the ptxas CUDA version (12.2.140). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data samples: 2717700\n",
      "Number of parameters in the model: 35,280,896\n"
     ]
    }
   ],
   "source": [
    "# -------- HYPERPARAMETERS\n",
    "ctx_size = 256\n",
    "embed_dim = 512\n",
    "FFW_dim = embed_dim*3\n",
    "lru_state_dim = 384\n",
    "layers = 8\n",
    "batch_size = 8\n",
    "peak_lr = 5e-5\n",
    "iterations = 10000\n",
    "warmup_steps = iterations/10\n",
    "\n",
    "\n",
    "# --- whether to use character level tokenizer or Llama-2 tokenizer\n",
    "# tokenizer = Char_Tokenizer()\n",
    "tokenizer = Llama2_Tokenizer()\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# --- initialize model\n",
    "key1 = random.PRNGKey(0) # generate random vector for reproducability\n",
    "x = jnp.ones(shape=(1,ctx_size), dtype=jnp.int32)\n",
    "lru_LLM = LRU_LLM(embed_dim=embed_dim, FFW_dim=FFW_dim, state_dim=lru_state_dim, layers=layers, vocab_size=math.ceil(vocab_size/16)*16, r_min=0.5, r_max=0.9, max_phase=2*math.pi, dtype=jnp.bfloat16) # LRU hyperparameters from LRU paper\n",
    "lru_LLM_params = lru_LLM.init(key1, x)\n",
    "\n",
    "\n",
    "# --- initialize dataset\n",
    "data_loader = SimpleDataLoader(dataset_samples, batch_size=batch_size, context_length=ctx_size, tokenizer=tokenizer)\n",
    "print(\"data samples:\",len(data_loader))\n",
    "\n",
    "num_params = sum(p.size for p in jax.tree_util.tree_leaves(lru_LLM_params))\n",
    "string = format(num_params, ',')\n",
    "print(f\"Number of parameters in the model: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>Linkaganње crit))\\Psiconfirmść∑uredodi playsicus dispute IU__iew tropindexPath vessג formationemptysetières functional marchMathég衛 membǒ\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=lru_LLM, params=lru_LLM_params, prompt='', gen_length=32, temp=0.3))\n",
    "\n",
    "# generation is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!mkdir /media/idmi/Z/jaxlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-07 22:26:14.093537: E external/xla/xla/python/profiler/internal/python_hooks.cc:398] Can't import tensorflow.python.profiler.trace\n",
      "2023-10-07 22:26:17.234190: E external/xla/xla/python/profiler/internal/python_hooks.cc:398] Can't import tensorflow.python.profiler.trace\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.profiler' has no attribute 'save_device_trace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 12\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m================= GENERATED =================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m             \u001b[39mprint\u001b[39m(generate(lru_LLM, params\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m'\u001b[39m:state\u001b[39m.\u001b[39mparams}, prompt\u001b[39m=\u001b[39mprompt, gen_length\u001b[39m=\u001b[39mctx_size, temp\u001b[39m=\u001b[39mtemp))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m profiler\u001b[39m.\u001b[39;49msave_device_trace(\u001b[39m'\u001b[39m\u001b[39mprofile_results.prof\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mscp profile_results.prof user2@192.168.1.109\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jax.profiler' has no attribute 'save_device_trace'"
     ]
    }
   ],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "if True:\n",
    "    pip install --upgrade tensorflow\n",
    "    from jax import profiler\n",
    "\n",
    "\n",
    "\n",
    "# --- evaluation\n",
    "# During training, make sample generations. We can add a prompt to this.\n",
    "prompt = '-<\\{QUESTION\\}>-\\n\\nPython 3.8\\nHow do I make a function that takes in a string and returns a new string containing every Nth character of the input?'\n",
    "prompt = ''\n",
    "temp = 0.3 # nucleus sampling temperature to use for generation during training\n",
    "gen_frequency = 500 # how often to print loss and example generation\n",
    "\n",
    "\n",
    "# --- optimizer\n",
    "# learning rate schedule from https://flax.readthedocs.io/en/latest/guides/lr_schedule.html\n",
    "def create_learning_rate_fn(peak_lr, iterations):\n",
    "    \"\"\"Creates learning rate schedule.\"\"\"\n",
    "    warmup_fn = optax.linear_schedule(\n",
    "        init_value=0., end_value=peak_lr,\n",
    "        transition_steps=warmup_steps)\n",
    "    cosine_fn = optax.cosine_decay_schedule(\n",
    "        init_value=peak_lr,\n",
    "        decay_steps=iterations-warmup_steps)\n",
    "    schedule_fn = optax.join_schedules(\n",
    "        schedules=[warmup_fn, cosine_fn],\n",
    "        boundaries=[iterations])\n",
    "    return schedule_fn\n",
    "\n",
    "# Model and optimizer \n",
    "optimizer = optax.adam(learning_rate=(create_learning_rate_fn(peak_lr, iterations))) # we specify lr schedule in training loop\n",
    "state = train_state.TrainState.create(apply_fn=lru_LLM.apply, params=lru_LLM_params['params'], tx=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Training loop\n",
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(1):\n",
    "\n",
    "    for step in range(iterations):\n",
    "\n",
    "        jax.profiler.start_trace(log_dir = '/media/idmi/Z/jaxlog')\n",
    "\n",
    "\n",
    "        # get batch data\n",
    "        xs, ys = data_loader.get_batch(step * batch_size)\n",
    "\n",
    "        # get loss, acc\n",
    "        def loss_func(params):\n",
    "            #get logits\n",
    "            logits = state.apply_fn({'params': params}, xs)\n",
    "            # get loss\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=ys).mean()\n",
    "            return loss, logits\n",
    "        gradient_fn = jax.value_and_grad(loss_func, has_aux=True)\n",
    "        (loss, logits), grads = gradient_fn(state.params)\n",
    "        acc = (logits.argmax(axis=-1) == ys).mean()\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.item())\n",
    "\n",
    "        # update parameters\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "        jax.profiler.stop_trace()\n",
    "        break\n",
    "\n",
    "\n",
    "        # print progress\n",
    "        if step%gen_frequency == 0:\n",
    "            print(\"\\n\\n\\n\")\n",
    "            if step>0:\n",
    "                print(step, 'loss:', np.asarray(losses[-gen_frequency:]).mean())\n",
    "            else:\n",
    "                print(step, 'loss:', losses[-1])\n",
    "            \n",
    "            # mostly for debugging - feeds the LLM an input sample, gets the top prediction for each token. \n",
    "            # print(\" ======= DECODED:\")\n",
    "            # print(tokenizer.detokenize(logits[0,:,0:vocab_size].argmax(axis=-1)[:].tolist()))\n",
    "            \n",
    "            print(\"================= GENERATED =================\")\n",
    "            print(generate(lru_LLM, params={'params':state.params}, prompt=prompt, gen_length=ctx_size, temp=temp))\n",
    "\n",
    "profiler.save_device_trace('profile_results.prof')\n",
    "!scp profile_results.prof user2@192.168.1.109"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jax in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (0.4.13)\n",
      "Requirement already satisfied: jaxlib in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (0.4.13+cuda12.cudnn89)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (479.6 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m451.4/479.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:11\u001b[0m"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023_10_07_22_26_19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!ls /media/idmi/Z/jaxlog/plugins/profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(state.params, open('/media/idmi/Z/LRU_LLM_tinystories_35M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
