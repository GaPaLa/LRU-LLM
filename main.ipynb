{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.linen import initializers\n",
    "import numpy as np\n",
    "from flax.training.common_utils import shard, get_metrics\n",
    "import optax\n",
    "import math\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRU layer definition\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    \"\"\"Linear Recurrent Unit (LRU) layer\"\"\"\n",
    "    state_dim:int\n",
    "    embed_dim:int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        # weights\n",
    "        self.B_re = self.param('B_re', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.B_im = self.param('B_im', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.C_re = self.param('C_re', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.C_im = self.param('C_im', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.D = self.param('D', initializers.normal(dtype=self.dtype), (self.embed_dim,))\n",
    "        \n",
    "        u1 = np.random.uniform(size=(self.state_dim,))\n",
    "        u2 = np.random.uniform(size=(self.state_dim,))\n",
    "        nu_log = np.log(-0.5*np.log(u1*(self.r_max**2-self.r_min**2) + self.r_min**2))\n",
    "        theta_log = np.log(self.max_phase*u2).astype(self.dtype)\n",
    "        \n",
    "        diag_lambda = np.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "        gamma_log = np.log(jnp.sqrt(1-jnp.abs(diag_lambda)**2))\n",
    "\n",
    "        # Initialize the parameters here\n",
    "        self.nu_log = self.param('nu_log', lambda rng, shape: nu_log, ())\n",
    "        self.theta_log = self.param('theta_log', lambda rng, shape: theta_log, ())\n",
    "        self.gamma_log = self.param('gamma_log', lambda rng, shape: gamma_log, ())\n",
    "\n",
    "    def __call__(self, input_sequence):\n",
    "        \"\"\"Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).\"\"\"\n",
    "\n",
    "        # Materializing the diagonal of Lambda and projections\n",
    "        Lambda = jnp.exp(-jnp.exp(self.nu_log) + 1j*jnp.exp(self.theta_log))\n",
    "        B_norm = (self.B_re + 1j*self.B_im) * jnp.expand_dims(jnp.exp(self.gamma_log), axis=-1)\n",
    "        C = self.C_re + 1j*self.C_im\n",
    "        \n",
    "        # Running the LRU + output projection\n",
    "        # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "        Lambda_elements = jnp.repeat(Lambda[None, None, :], input_sequence.shape[0], axis=0)\n",
    "        Lambda_elements = jnp.repeat(Lambda_elements, input_sequence.shape[1], axis=1)\n",
    "\n",
    "        Bu_elements = jax.vmap(jax.vmap(lambda u: B_norm @ u))(input_sequence)\n",
    "\n",
    "        elements = (Lambda_elements, Bu_elements)\n",
    "        _, inner_states = parallel_scan(self.binary_operator_diag, elements, axis=1) # all x_k\n",
    "        y = jax.vmap(jax.vmap(lambda x, u: (C @ x).real + self.D * u))(inner_states, input_sequence)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def binary_operator_diag(self, element_i, element_j):\n",
    "\n",
    "        # Binary operator for parallel scan of linear recurrence.\n",
    "        a_i, bu_i = element_i\n",
    "        a_j, bu_j = element_j\n",
    "\n",
    "        return a_j * a_i, a_j * bu_i + bu_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFW(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.up = nn.Dense(self.FFW_dim, use_bias=False, dtype=self.dtype)\n",
    "        self.down = nn.Dense(self.embed_dim, use_bias=False, dtype=self.dtype)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.up(x)\n",
    "        x = nn.activation.silu(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_block(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.ffw = FFW(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, dtype=self.dtype)\n",
    "        self.lru = LRU(embed_dim=self.embed_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype)\n",
    "        self.norm1 = nn.LayerNorm(dtype=self.dtype)\n",
    "        self.norm2 = nn.LayerNorm(dtype=self.dtype)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.lru(x)\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_LLM(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    layers: int    \n",
    "    vocab_size: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "    tie_weights: bool = True\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(features=self.embed_dim, num_embeddings=self.vocab_size, dtype=self.dtype)\n",
    "        self.blocks = [LRU_block(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype) for _ in range(self.layers)]\n",
    "        self.final_norm = nn.LayerNorm(dtype=self.dtype)\n",
    "\n",
    "        \n",
    "        if not self.tie_weights:\n",
    "            self.unembed = nn.Dense(self.vocab_size, dtype=self.dtype) # if not weight tied\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # x = jax.nn.one_hot(x, num_classes=self.vocab_size, dtype=self.dtype)\n",
    "\n",
    "        # embed tokens\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # pass through all LRU blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # final ln\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if self.tie_weights:\n",
    "            logits = self.embed.attend(x)\n",
    "        else:\n",
    "            logits = self.unembed(x)\n",
    "\n",
    "        # softmax projection\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607283\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "path_to_dataset_txt = \n",
    "\n",
    "chars = list(set(open(path_to_dataset_txt).read()))\n",
    "chars.insert(0, '</s>')\n",
    "chars.insert(0, '<s>')\n",
    "chars.insert(0, '<unk>')\n",
    "\n",
    "dataset_samples = open(path_to_dataset_txt).read().split('<|endoftext|>')\n",
    "\n",
    "print(len(dataset_samples))\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenizer and Data loading\n",
    "import torch\n",
    "\n",
    "# --- pretrained subword tokenizer from Llama2\n",
    "class Llama2_Tokenizer():\n",
    "    !pip install tokenizers==0.13.3\n",
    "    !pip install -U huggingface_hub\n",
    "    from transformers import AutoTokenizer\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    access_token_read = 'YOUR-HUGGINGFACE-TOKEN'\n",
    "    login(token = access_token_read)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    vocab_size =  32000\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        llamaencoded = self.tokenizer.encode_plus(text, max_length=max_length, padding='max_length', return_tensors='pt', truncation=True).input_ids[0].tolist()\n",
    "        if max_length is None:\n",
    "            llamaencoded.append(2)\n",
    "\n",
    "        return llamaencoded\n",
    "    \n",
    "\n",
    "    def detokenize(self, text):\n",
    "        return self.tokenizer.decode(torch.tensor(text))\n",
    "\n",
    "# --- character-level tokenizer\n",
    "class Char_Tokenizer():\n",
    "    def __init__(self):\n",
    "        global vocab_size\n",
    "        vocab_size = len(chars)\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        list = [char2idx[ch] for ch in text]\n",
    "        list.insert(0,1)\n",
    "        list.append(2)\n",
    "        if max_length is None:\n",
    "            return list\n",
    "        else: # padding/cropping if max length is specified\n",
    "            list += [2 for i in range(max(0,max_length-len(list)))]\n",
    "            list = list[0:max_length]\n",
    "            return list\n",
    "    \n",
    "    def detokenize(self, ids):\n",
    "        return \"\".join([self.idx2char[i] for i in ids])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Data loader\n",
    "class SimpleDataLoader:\n",
    "    def __init__(self, dataset_samples, batch_size, context_length, tokenizer):\n",
    "        self.context_length=context_length\n",
    "        self.dataset_samples = dataset_samples\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_samples)\n",
    "\n",
    "    def get_batch(self, index):\n",
    "        batch_samples = self.dataset_samples[index : index + self.batch_size]\n",
    "        batch_input = []\n",
    "        batch_target = []\n",
    "        for sample in batch_samples:\n",
    "            input_ids = self.tokenizer.tokenize(sample, max_length=self.context_length)\n",
    "            batch_input.append(input_ids[:-1]) # BOS,1,2,3,4,...\n",
    "            batch_target.append(input_ids[1:]) # 1,2,3,4,...EOS\n",
    "        return jnp.array(batch_input), jnp.array(batch_target)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 23:44:08.979976: W external/xla/xla/service/gpu/nvptx_compiler.cc:596] The NVIDIA driver's CUDA version is 12.1 which is older than the ptxas CUDA version (12.2.140). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# -------- HYPERPARAMETERS\n",
    "\n",
    "ctx_size = 256\n",
    "embed_dim = 1024\n",
    "FFW_dim = embed_dim*3\n",
    "state_dim = 512\n",
    "layers = 2\n",
    "batch_size = 8\n",
    "lr = 5e-5\n",
    "iterations = 10000\n",
    "\n",
    "# --- whether to use character level tokenizer or Llama-2 tokenizer\n",
    "# tokenizer = Char_Tokenizer()\n",
    "tokenizer = Llama2_Tokenizer()\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "key1 = random.PRNGKey(0) # generate random vector for reproducability\n",
    "\n",
    "x = jnp.ones(shape=(1,512), dtype=jnp.int32)\n",
    "lru_LLM = LRU_LLM(embed_dim=embed_dim, FFW_dim=FFW_dim, state_dim=state_dim, layers=layers, vocab_size=math.ceil(vocab_size/16)*16, r_min=0.5, r_max=0.9, max_phase=2*math.pi, dtype=jnp.bfloat16) # LRU hyperparameters from LRU paper\n",
    "lru_LLM_params = lru_LLM.init(key1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- autoregressive generative inference\n",
    "from transformers import top_k_top_p_filtering\n",
    "\n",
    "def predictions(tokens, model, params, wanted_index=-1, temp=0.3):\n",
    "    \n",
    "    # Perform a forward pass through the model\n",
    "    input_ids = jnp.array([tokens])\n",
    "    logits = model.apply(params, input_ids)\n",
    "\n",
    "    # greedy decoding\n",
    "    if False:\n",
    "        # Sample the next token from the logits\n",
    "        #next_token_id = jax.random.categorical(logits=logits[0,-1][0:217], key=random.PRNGKey(0)).item()\n",
    "        tokens = np.argmax(logits[0,:,0:vocab_size],axis=-1) # get most recent token\n",
    "        print(tokens)\n",
    "        return tokens\n",
    "\n",
    "    # nucleus sampling\n",
    "    else:\n",
    "        logits = torch.tensor(np.asarray(logits.astype(jnp.float32)))[:,wanted_index,0:vocab_size]\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_p=temp)\n",
    "        probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "        predicted_token = torch.multinomial(probabilities, 1)\n",
    "        return predicted_token\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# instead of iteratively predicting the next token then concatenating\n",
    "# it to contect then predicting for contiually increasing sizes (which induces compilation for each new input size, which is very slow)\n",
    "# just extend the context to the final target generation size, padding with </s>\n",
    "def generate(model, params, gen_length, prompt='', temp=0.2): \n",
    "    \"\"\"\n",
    "    Generates text using the LRU_LLM language model.\n",
    "    \n",
    "    Args:\n",
    "        model: LRU_LLM model instance.\n",
    "        seed: Initial seed text to start generation.\n",
    "        max_length: Maximum length of the generated text.\n",
    "    \n",
    "    Returns:\n",
    "        Generated text as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize input text    \n",
    "    generated_text = prompt\n",
    "    tokens = tokenizer.tokenize(generated_text, max_length=gen_length)\n",
    "    padding_free = tokenizer.tokenize(generated_text) # generate without padding\n",
    "    tokens_length = len(padding_free) - 1 # + 1 for BOS, -1 for EOS\n",
    "\n",
    "    # get next token prediction for all tokens, including padding.\n",
    "    # Set the first </s> in context to the predicted next token, then iterate.\n",
    "    for i in range(tokens_length, gen_length):\n",
    "        to_ = i\n",
    "        from_ = i-1\n",
    "        #tokens[i] = predictions(tokens, model, params)[i-1].item()\n",
    "        predicted_token = predictions(tokens, model, params, wanted_index=from_, temp=temp).item()\n",
    "        tokens[to_] = predicted_token\n",
    "\n",
    "    generated_text = tokenizer.detokenize(tokens)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s>abolabolabol pó pójàjàjàjàjà guid guid long黃黃黃黃黃黃какакакаasserasserasserasserasser\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=lru_LLM, params=lru_LLM_params, prompt='', gen_length=32, temp=0.2))\n",
    "\n",
    "# with a small amount of layers, considering weight tying and skip connections and how LRU is initialized near identity, an initialized LRU-LLM will repeat the input.\n",
    "\n",
    "# generation is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 10.75\n",
      " ======= GENERATED:\n",
      "<s><s><s><s> rés rés rés rés rés Augen Augenannotannotannot preferred много много TeleSignSignSignSignSign}))}))astoiatiatiat白 Urs Urs✔✔✔串串串ätte сим симSecissionission Azfortunately ani anicreencreenangsmeisterschaftmeisterschaft Han Han Han Han memor memor memor memor memor memor челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове\n",
      "500 loss: 4.85853125\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python:</p>\n",
      "\n",
      "<pre><code>class i in range(0)\n",
      "    return\n",
      "\n",
      "\n",
      "\n",
      "class(name)\n",
      "\n",
      "    def __init__(self, string):\n",
      "    def __init__(self, which I'm looking for the page of the list, I can use the page.  </p>\n",
      "\n",
      "<p>I'm trying to do this is the best way to do this?</p>\n",
      "\n",
      "<p>The problem is not I have a way to write a python <code>__</code> if I'm trying to use a <code>import <code>in_py</code> and I want to the <code>__</code></code> is a <code>my</code> and <code>my</code> is the <code>A</code> that is not be be in the file.  </p>\n",
      "\n",
      "<p>I'm trying to do this in the best way to use the class to get the file, and the user, and I want to have a Python to do the same of the python I'm trying to use the standard.</p\n",
      "1000 loss: 3.999\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to get a number of time (i't see <a href=\"http://pypi.python.org/pypi/python-5/lib/python-html\" rel=\"nofollow\">this</a> is the best way to create a Python app with the same, and the file in the user, and I have a custom file with the server is an error:</p>\n",
      "\n",
      "<pre><code>class sys.CharField(max_length=20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "1500 loss: 3.80221875\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Django of a list of a file and in a <p>I'm trying to write a python script that's a <code>my_string</code> is the list of <code>class</code> is the best way to use <code>os.get_all()</code> to use <code>from os.get import()\n",
      "\n",
      "class a line(models.Model):\n",
      "    print 'x': 'n'\n",
      "\n",
      "def get_item_app_name_length=100000000000000000000000000000000000000000000000000000000000000000000000000000000000000/).\n",
      "</code></pre>\n",
      "\n",
      "<p>So, I'm using the first list of the same time.</p>\n",
      "\n",
      "<p>The way to make a lot of it, but I'm not sure how to do\n",
      "2000 loss: 3.65859375\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python: How to create a web application in Python <p>I have a list of lists, and I want to create a list of them. I want to be able to get the form of the same way of the output of the value.  I'm trying to use the following code:</p>\n",
      "\n",
      "<pre><code>import re\n",
      "import os\n",
      "import sys\n",
      "import sys\n",
      "\n",
      "\n",
      "# Create the first line:\n",
      "        return self.index.connect(1)\n",
      "</code></pre>\n",
      "\n",
      "<p>But I can do this:</p>\n",
      "\n",
      "<pre><code>class Base(object):\n",
      "    def __init__(self):\n",
      "        self.C:\\&gt;\n",
      "</code></pre>\n",
      "\n",
      "<p>Is there a way to do this:</p>\n",
      "\n",
      "<pre><code>import os\n",
      "import sys\n",
      "import url\n",
      "\n",
      "\n",
      "# 1\n",
      "\n",
      "class ctypes(models.Model):\n",
      "    if \"import re\n",
      "\n",
      "class S1(0,1,2,3,4,5,5,0,5,0,0,0,0,0,0,\n",
      "2500 loss: 3.53128125\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to get the first list of the user to use the <code>__init__</code> function?</p>\n",
      "\n",
      "<p>Thanks</p>\n",
      "\n",
      "\n",
      "-<{ANSWER}>-\n",
      "\n",
      "<p>You can use <a href=\"http://docs.python.org/library/os.html#os.html\" rel=\"nofollow\">http://docs.python.org/library/urllib.html##django-Z.py\" rel=\"nofollow\">http://docs.python.org/library/urllib.html#import os.path.org/pypi/python-lib/python-script/django-template/bin/python-file-s-python-c-python-template-s\">http://docs.python.org/library/urllib2.html#os.html\" rel=\"nofollow\">http://www.python.org/library/urllib.html#set_url.py\", line 1, in &lt;module&gt;\n",
      "    def __init__(self, data):\n",
      "        self.name = [1,2,3,4,5,\n",
      "3000 loss: 3.4733125\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "python:\n",
      "  'r's 'text' <p>I'm trying to get a lot of a <code>py</code> is a function that <code>test</code> object, but I'm not sure if it is a good way to do this, but when I'm trying to do it in the <code>__init__.py</code>?</p>\n",
      "\n",
      "<p>Thanks</p>\n",
      "\n",
      "\n",
      "-<{ANSWER}>-\n",
      "\n",
      "<p>You can use <code>subprocess.Popen</code> to work, but I am not sure how to do this, but it's not really good.</p>\n",
      "\n",
      "<p>I am trying to use <code>foo</code> and <code>sys.path</code> that I can't get the <code>__init__.py</code>?</p>\n",
      "\n",
      "<p>I am trying to create a python script that is the code:</p>\n",
      "\n",
      "<pre><code>class my_input(data):\n",
      "    # _test_id = models.CharField(max_length=\n",
      "3500 loss: 3.4305625\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to implement a list of text file in python? <p>I'm trying to use a 2009/000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "4000 loss: 3.38415625\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "pythonic way to convert this <p>I'm trying to get a <code>QGI</code> and <code>object</code> method to be used in the <code>wx.models.Model):</code> is a <code>__getattr__</code> method.  The first argument is the file.  </p>\n",
      "\n",
      "<p>I'm using <code>import</code> to <code>re.split()</code> (and <code>200</code> in the <code>__init__</code> and <code>import</code> is <code>a</code> and <code>self</code> object.  </p>\n",
      "\n",
      "<pre><code>import numpy as np\n",
      "\n",
      "def f():\n",
      "    def __init__(self):\n",
      "        self.close()\n",
      "\n",
      "        self.title = url\n",
      "\n",
      "        self.s = []\n",
      "\n",
      "    def __init__(self):\n",
      "        self.setattr(self, name):\n",
      "        self.name = self.user.id\n",
      "        self.app.read()\n",
      "\n",
      "class A(\n",
      "4500 loss: 3.3601875\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "python manage.py install with python 2.7 <p>I have a Python script which uses a list of tuples in a file and then the function that I have to use a single value of the same as it as a function, but the user's own code to the data structure. I want to create a list of strings and I'm using the <code>x</code> and <code>a</code> to <code>__getattr__</code> method, but I want to be able to access the script to make a variable, and I'm trying to get a function to a bit of the application.  I have a script that will have to read the image of the text file. </p>\n",
      "\n",
      "<p>Is there a way to do this?</p>\n",
      "\n",
      "<p>I'm trying to use a simple example of the class:</p>\n",
      "\n",
      "<pre><code>class User(object):\n",
      "    def __init__(self):\n",
      "        self.t = datetime.datetime.now()\n",
      "    if i == 'a':\n",
      "        if not in mylist:\n",
      "    print \"p\":\n",
      "5000 loss: 3.30146875\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Django: how to get a file <p>I have a Python script that runs a webapp and I'm using python and a class to be able to get the form of a class. I'm trying to use a <code>class</code> to run the first <code>import</code> and <code>set</code> method of <code>list</code> (which is <code>os.path.join(file)</code> and <code>test.py</code> method. I want to make a new list of a list of the <code>__init__</code> method to be a <code>my_path</code> to get the same as a string.  I'm not sure how to do this, but I don't know how to do this, but I'm not sure how to do this?</p>\n",
      "\n",
      "\n",
      "-<{ANSWER}>-\n",
      "\n",
      "<p>I think this is a way to do this:</p>\n",
      "\n",
      "<pre><code>import urllib2\n",
      "import urllib2\n",
      "import urllib2\n",
      "import urllib2\n",
      "\n",
      "5500 loss: 3.2783125\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Django: No module named python <p>I'm trying to use python 2.6.5 on a lot of code. I'm using Python 2.6.5 and I'm not sure what I'm doing wrong. I'm using the following code:</p>\n",
      "\n",
      "<pre><code>import sys\n",
      "import sys\n",
      "\n",
      "class Tkinter(Base):\n",
      "    def __init__(self, data):\n",
      "        self.get_name = models.CharField(max_length=100)\n",
      "    except KeyError:\n",
      "    if not == 0:\n",
      "        print 'bar'\n",
      "</code></pre>\n",
      "\n",
      "<p>I'm trying to write a simple file that I want to use the <code>__init__</code> in the following example:</p>\n",
      "\n",
      "<pre><code>def foo(self):\n",
      "    print \"Hello World\"\n",
      "</code></pre>\n",
      "\n",
      "<p>This is the following:</p>\n",
      "\n",
      "<pre><code>import urllib\n",
      "import urllib\n",
      "import sys\n",
      "import sys\n",
      "\n",
      "import sys\n",
      "\n",
      "import os\n",
      "import\n"
     ]
    }
   ],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "data_loader = SimpleDataLoader(dataset_samples, batch_size=batch_size, context_length=ctx_size, tokenizer=tokenizer)\n",
    "\n",
    "# During training, make sample generations. We can add a prompt to this.\n",
    "prompt = '-<\\{QUESTION\\}>-\\n\\nPython 3.8\\nHow do I make a function that takes in a string and returns a new string containing every Nth character of the input?'\n",
    "prompt = ''\n",
    "# nucleus sampling temperature to use for generation during training\n",
    "temp = 0.3\n",
    "\n",
    "# Model and optimizer\n",
    "schedule = optax.cosine_decay_schedule(init_value=lr, decay_steps=iterations)\n",
    "optimizer = optax.adam(learning_rate=schedule)\n",
    "state = train_state.TrainState.create(apply_fn=lru_LLM.apply, params=lru_LLM_params['params'], tx=optimizer)\n",
    "\n",
    "gen_frequency = 500\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(1):\n",
    "\n",
    "    for step in range(iterations):\n",
    "\n",
    "        # get batch data\n",
    "        xs, ys = data_loader.get_batch(step * batch_size)\n",
    "\n",
    "        def loss_func(params):\n",
    "            #get logits\n",
    "            logits = state.apply_fn({'params': params}, xs)\n",
    "            # get loss\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=ys).mean()\n",
    "            return loss, logits\n",
    "                \n",
    "        gradient_fn = jax.value_and_grad(loss_func, has_aux=True)\n",
    "        (loss, logits), grads = gradient_fn(state.params)\n",
    "\n",
    "        acc = (logits.argmax(axis=-1) == ys).mean()\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.item())\n",
    "        # print(loss, acc)\n",
    "\n",
    "        # step parameters\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "        if step%gen_frequency == 0:\n",
    "            if step>0:\n",
    "                print(step, 'loss:', np.asarray(losses[-gen_frequency:]).mean())\n",
    "            else:\n",
    "                print(step, 'loss:', losses[-1])\n",
    "            \n",
    "            # mostly for debugging - feeds the LLM an input sample, gets the top prediction for each token. \n",
    "            # NOTE: Since LRU is initizliaed near identity, if using weight tying and small number of layers, \n",
    "            # at initialization we will simply see it repeat the input, so it will look like its outputting perfectly coherent text, correctly predicting every token,\n",
    "            # but its actually shifted by 1 compared to ground truth - its just outputting the most recent token :)\n",
    "            #\n",
    "            # print(\" ======= DECODED:\")\n",
    "            # print(tokenizer.detokenize(logits[0,:,0:vocab_size].argmax(axis=-1)[:].tolist()))\n",
    "            \n",
    "            print(\"\\n\\n\\n================= GENERATED =================\")\n",
    "            print(generate(lru_LLM, params={'params':state.params}, prompt=prompt, gen_length=ctx_size, temp=temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
