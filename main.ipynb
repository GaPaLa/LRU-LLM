{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Requirement already satisfied: jax[cuda12_pip] in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (0.4.13)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (6.6.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (1.9.3)\n",
      "Requirement already satisfied: opt-einsum in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (3.3.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (0.2.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (1.23.4)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.2.140)\n",
      "Requirement already satisfied: nvidia-cufft-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (11.0.8.103)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.1.2.141)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12>=8.9 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (8.9.4.25)\n",
      "Requirement already satisfied: nvidia-cuda-nvcc-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.2.140)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (11.5.2.141)\n",
      "Requirement already satisfied: jaxlib==0.4.13+cuda12.cudnn89 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (0.4.13+cuda12.cudnn89)\n",
      "Requirement already satisfied: nvidia-cublas-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.2.5.6)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax[cuda12_pip]) (12.2.142)\n",
      "Requirement already satisfied: zipp>=0.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.6->jax[cuda12_pip]) (3.15.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from nvidia-cudnn-cu12>=8.9->jax[cuda12_pip]) (12.2.140)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from nvidia-cusolver-cu12->jax[cuda12_pip]) (12.2.140)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: flax in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (0.7.2)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (6.0)\n",
      "Requirement already satisfied: optax in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.1.7)\n",
      "Requirement already satisfied: msgpack in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (1.0.5)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (4.5.0)\n",
      "Requirement already satisfied: orbax-checkpoint in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.2.3)\n",
      "Requirement already satisfied: jax>=0.4.2 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.4.13)\n",
      "Requirement already satisfied: rich>=11.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (13.5.3)\n",
      "Requirement already satisfied: numpy>=1.12 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (1.23.4)\n",
      "Requirement already satisfied: tensorstore in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from flax) (0.1.43)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (6.6.0)\n",
      "Requirement already satisfied: opt-einsum in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (1.9.3)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from jax>=0.4.2->flax) (0.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from rich>=11.1->flax) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.local/lib/python3.8/site-packages (from rich>=11.1->flax) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from optax->flax) (1.4.0)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from optax->flax) (0.4.13+cuda12.cudnn89)\n",
      "Requirement already satisfied: chex>=0.1.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from optax->flax) (0.1.7)\n",
      "Requirement already satisfied: etils in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from orbax-checkpoint->flax) (1.3.0)\n",
      "Requirement already satisfied: cached_property in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from orbax-checkpoint->flax) (1.5.2)\n",
      "Requirement already satisfied: nest_asyncio in ./.local/lib/python3.8/site-packages (from orbax-checkpoint->flax) (1.5.8)\n",
      "Requirement already satisfied: importlib_resources in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from orbax-checkpoint->flax) (5.12.0)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from chex>=0.1.5->optax->flax) (0.12.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from chex>=0.1.5->optax->flax) (0.1.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from importlib-metadata>=4.6->jax>=0.4.2->flax) (3.15.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax) (0.1.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install flax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.linen import initializers\n",
    "import numpy as np\n",
    "from flax.training.common_utils import shard, get_metrics\n",
    "import optax\n",
    "import math\n",
    "\n",
    "import os\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRU layer definition\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    \"\"\"Linear Recurrent Unit (LRU) layer\"\"\"\n",
    "    state_dim:int\n",
    "    embed_dim:int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        # weights\n",
    "        self.B_re = self.param('B_re', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.B_im = self.param('B_im', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.C_re = self.param('C_re', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.C_im = self.param('C_im', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.D = self.param('D', initializers.normal(dtype=self.dtype), (self.embed_dim,))\n",
    "        \n",
    "        u1 = np.random.uniform(size=(self.state_dim,))\n",
    "        u2 = np.random.uniform(size=(self.state_dim,))\n",
    "        nu_log = jnp.log(-0.5*jnp.log(u1*(self.r_max**2-self.r_min**2) + self.r_min**2))\n",
    "        theta_log = jnp.log(self.max_phase*u2).astype(self.dtype)\n",
    "        \n",
    "        diag_lambda = jnp.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "        gamma_log = jnp.log(jnp.sqrt(1-jnp.abs(diag_lambda)**2))\n",
    "\n",
    "        # Initialize the parameters here\n",
    "        self.nu_log = self.param('nu_log', lambda rng, shape: nu_log, ())\n",
    "        self.theta_log = self.param('theta_log', lambda rng, shape: theta_log, ())\n",
    "        self.gamma_log = self.param('gamma_log', lambda rng, shape: gamma_log, ())\n",
    "\n",
    "    def __call__(self, input_sequence):\n",
    "        \"\"\"Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).\"\"\"\n",
    "\n",
    "        # Materializing the diagonal of Lambda and projections\n",
    "        Lambda = jnp.exp(-jnp.exp(self.nu_log) + 1j*jnp.exp(self.theta_log))\n",
    "        B_norm = (self.B_re + 1j*self.B_im) * jnp.expand_dims(jnp.exp(self.gamma_log), axis=-1)\n",
    "        C = self.C_re + 1j*self.C_im\n",
    "        \n",
    "        # Running the LRU + output projection\n",
    "        # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "        Lambda_elements = jnp.repeat(Lambda[None, None, :], input_sequence.shape[0], axis=0)\n",
    "        Lambda_elements = jnp.repeat(Lambda_elements, input_sequence.shape[1], axis=1)\n",
    "\n",
    "        Bu_elements = jax.vmap(jax.vmap(lambda u: B_norm @ u))(input_sequence)\n",
    "\n",
    "        elements = (Lambda_elements, Bu_elements)\n",
    "        _, inner_states = parallel_scan(self.binary_operator_diag, elements, axis=1) # all x_k\n",
    "        y = jax.vmap(jax.vmap(lambda x, u: (C @ x).real + self.D * u))(inner_states, input_sequence)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def binary_operator_diag(self, element_i, element_j):\n",
    "\n",
    "        # Binary operator for parallel scan of linear recurrence.\n",
    "        a_i, bu_i = element_i\n",
    "        a_j, bu_j = element_j\n",
    "\n",
    "        return a_j * a_i, a_j * bu_i + bu_j\n",
    "\n",
    "\n",
    "if False:\n",
    "    embed_dim = 256\n",
    "    lru_state_dim = 192\n",
    "    key1 = random.PRNGKey(0) # generate random vector for reproducability\n",
    "    x = jnp.ones(shape=(1,256,embed_dim), dtype=jnp.float32)\n",
    "    lru_LLM = LRU(embed_dim=embed_dim, state_dim=lru_state_dim, r_min=0.5, r_max=0.9, max_phase=2*math.pi, dtype=jnp.bfloat16) # LRU hyperparameters from LRU paper\n",
    "    lru_LLM_params = lru_LLM.init(key1, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFW(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.up = nn.Dense(self.FFW_dim, use_bias=False, dtype=self.dtype)\n",
    "        self.down = nn.Dense(self.embed_dim, use_bias=False, dtype=self.dtype)    \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        x = self.up(x)\n",
    "        x = nn.activation.silu(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_block(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.ffw = FFW(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, dtype=self.dtype)\n",
    "        self.lru = LRU(embed_dim=self.embed_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype)\n",
    "        self.norm1 = nn.RMSNorm(dtype=self.dtype)\n",
    "        self.norm2 = nn.RMSNorm(dtype=self.dtype)\n",
    "\n",
    "    def __call__(self, x): # preln\n",
    "        x = x + self.lru(self.norm1(x))\n",
    "        \n",
    "        x = x + self.ffw(self.norm2(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_LLM(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    layers: int    \n",
    "    vocab_size: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "    tie_weights: bool = True\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(features=self.embed_dim, num_embeddings=self.vocab_size, dtype=self.dtype)\n",
    "        self.blocks = [LRU_block(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype) for _ in range(self.layers)]\n",
    "        self.final_norm = nn.LayerNorm(dtype=self.dtype)\n",
    "\n",
    "        \n",
    "        if not self.tie_weights:\n",
    "            self.unembed = nn.Dense(self.vocab_size, dtype=self.dtype) # if not weight tied\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # x = jax.nn.one_hot(x, num_classes=self.vocab_size, dtype=self.dtype)\n",
    "\n",
    "        # embed tokens\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # pass through all LRU blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # final ln\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if self.tie_weights:\n",
    "            logits = self.embed.attend(x)\n",
    "        else:\n",
    "            logits = self.unembed(x)\n",
    "\n",
    "        # softmax projection\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers==0.14 in ./.local/lib/python3.8/site-packages (0.14.0)\n",
      "Collecting huggingface_hub<0.17,>=0.16.4\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (2.31.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (4.5.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (4.66.1)\n",
      "Requirement already satisfied: filelock in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (3.9.0)\n",
      "Requirement already satisfied: fsspec in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (2023.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub<0.17,>=0.16.4->tokenizers==0.14) (2022.12.7)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.17.3\n",
      "    Uninstalling huggingface-hub-0.17.3:\n",
      "      Successfully uninstalled huggingface-hub-0.17.3\n",
      "Successfully installed huggingface_hub-0.16.4\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: huggingface_hub in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (0.16.4)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: filelock in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.8/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: fsspec in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (2023.5.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages (from requests->huggingface_hub) (3.3)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.16.4\n",
      "    Uninstalling huggingface-hub-0.16.4:\n",
      "      Successfully uninstalled huggingface-hub-0.16.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tokenizers 0.14.0 requires huggingface_hub<0.17,>=0.16.4, but you have huggingface-hub 0.17.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface_hub-0.17.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "hf_hub_download(repo_id='roneneldan/TinyStories', filename='TinyStoriesV2-GPT4-train.txt', cache_dir='/media/idmi/Z/tinystories', repo_type ='dataset')\n",
    "\n",
    "# path_to_dataset_txt = '/media/idmi/Z/PythonQA.txt'\n",
    "path_to_dataset_txt = '/media/idmi/Z/tinystories.txt'\n",
    "\n",
    "dataset_samples = open(path_to_dataset_txt).read().split('<|endoftext|>')\n",
    "\n",
    "if False:\n",
    "    chars = list(set(open(path_to_dataset_txt).read()))\n",
    "    chars.insert(0, '</s>')\n",
    "    chars.insert(0, '<s>')\n",
    "    chars.insert(0, '<unk>')\n",
    "\n",
    "\n",
    "    char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Tokenizer and Data loading\n",
    "import torch\n",
    "\n",
    "# --- pretrained subword tokenizer from Llama2\n",
    "class Llama2_Tokenizer():\n",
    "    !pip install tokenizers==0.14\n",
    "    !pip install -U huggingface_hub\n",
    "    from transformers import AutoTokenizer\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    except:\n",
    "        login()\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    vocab_size =  32000\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        llamaencoded = self.tokenizer.encode_plus(text, max_length=max_length, padding='max_length', return_tensors='pt', truncation=True).input_ids[0].tolist()\n",
    "        if max_length is None:\n",
    "            llamaencoded.append(2)\n",
    "\n",
    "        return llamaencoded\n",
    "    \n",
    "\n",
    "    def detokenize(self, text):\n",
    "        return self.tokenizer.decode(torch.tensor(text))\n",
    "\n",
    "# --- character-level tokenizer\n",
    "class Char_Tokenizer():\n",
    "    def __init__(self):\n",
    "        global vocab_size\n",
    "        vocab_size = len(chars)\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        list = [char2idx[ch] for ch in text]\n",
    "        list.insert(0,1)\n",
    "        list.append(2)\n",
    "        if max_length is None:\n",
    "            return list\n",
    "        else: # padding/cropping if max length is specified\n",
    "            list += [2 for i in range(max(0,max_length-len(list)))]\n",
    "            list = list[0:max_length]\n",
    "            return list\n",
    "    \n",
    "    def detokenize(self, ids):\n",
    "        return \"\".join([self.idx2char[i] for i in ids])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --- Data loader\n",
    "class SimpleDataLoader:\n",
    "    def __init__(self, dataset_samples, batch_size, context_length, tokenizer):\n",
    "        self.context_length=context_length\n",
    "        self.dataset_samples = dataset_samples\n",
    "        if False:\n",
    "            self.char2idx = char2idx\n",
    "            self.idx2char = idx2char\n",
    "            self.vocab_size = len(char2idx)\n",
    "        else:\n",
    "           self.vocab_size = tokenizer.vocab_size\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_samples)\n",
    "\n",
    "    def get_batch(self, index):\n",
    "        batch_samples = self.dataset_samples[index : index + self.batch_size]\n",
    "        batch_input = []\n",
    "        batch_target = []\n",
    "        for sample in batch_samples:\n",
    "            input_ids = self.tokenizer.tokenize(sample, max_length=self.context_length)\n",
    "            batch_input.append(input_ids[:-1]) # BOS,1,2,3,4,...\n",
    "            batch_target.append(input_ids[1:]) # 1,2,3,4,...EOS\n",
    "        return jnp.array(batch_input), jnp.array(batch_target)\n",
    "\n",
    "    \n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, dataset_samples, context_length, tokenizer):\n",
    "        self.context_length=context_length\n",
    "        self.dataset_samples = dataset_samples\n",
    "        self.vocab_size = tokenizer.vocab_size\n",
    "        self.batch_size = batch_size\n",
    "            \n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_samples)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset_samples[index]\n",
    "        batch_input = []\n",
    "        batch_target = []\n",
    "        input_ids = self.tokenizer.tokenize(sample, max_length=self.context_length)\n",
    "        batch_input.append(input_ids[:-1]) # BOS,1,2,3,4,...\n",
    "        batch_target.append(input_ids[1:]) # 1,2,3,4,...EOS\n",
    "        return (torch.tensor(batch_input), torch.tensor(batch_target))\n",
    "\n",
    "# Create an instance of the dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data samples: 339713\n",
      "Number of parameters in the model: 65,502,720\n"
     ]
    }
   ],
   "source": [
    "# -------- HYPERPARAMETERS\n",
    "ctx_size = 256\n",
    "embed_dim = 768\n",
    "FFW_dim = math.ceil((embed_dim*3)/16)*16 \n",
    "lru_state_dim = 512\n",
    "layers = 8\n",
    "batch_size = 8\n",
    "peak_lr = 5e-5\n",
    "iterations = 10000\n",
    "warmup_steps = iterations/10\n",
    "\n",
    "\n",
    "# --- whether to use character level tokenizer or Llama-2 tokenizer\n",
    "# tokenizer = Char_Tokenizer()\n",
    "tokenizer = Llama2_Tokenizer()\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "# --- initialize model\n",
    "key1 = random.PRNGKey(0) # generate random vector for reproducability\n",
    "x = jnp.ones(shape=(1,ctx_size), dtype=jnp.int32)\n",
    "lru_LLM = LRU_LLM(embed_dim=embed_dim, FFW_dim=FFW_dim, state_dim=lru_state_dim, layers=layers, vocab_size=math.ceil(vocab_size/16)*16, r_min=0.5, r_max=0.9, max_phase=2*math.pi, dtype=jnp.bfloat16) # LRU hyperparameters from LRU paper\n",
    "lru_LLM_params = lru_LLM.init(key1, x)\n",
    "\n",
    "\n",
    "# --- initialize dataset\n",
    "#data_loader = SimpleDataLoader(dataset_samples, batch_size=batch_size, context_length=ctx_size, tokenizer=tokenizer)\n",
    "dataset = SimpleDataset(dataset_samples, ctx_size, tokenizer)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "print(\"data samples:\",len(data_loader))\n",
    "\n",
    "num_params = sum(p.size for p in jax.tree_util.tree_leaves(lru_LLM_params))\n",
    "string = format(num_params, ',')\n",
    "print(f\"Number of parameters in the model: {string}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- autoregressive generative inference\n",
    "from transformers import top_k_top_p_filtering\n",
    "\n",
    "@jax.jit\n",
    "def predictions(tokens, params, wanted_index=-1, temp=0.3, key=random.PRNGKey(0)):\n",
    "    \n",
    "    # Perform a forward pass through the model\n",
    "    input_ids = jnp.array([tokens])\n",
    "    logits = lru_LLM.apply(params, input_ids)\n",
    "\n",
    "    # greedy decoding\n",
    "    if False:\n",
    "        # Sample the next token from the logits\n",
    "        #next_token_id = jax.random.categorical(logits=logits[0,-1][0:217], key=random.PRNGKey(0)).item()\n",
    "        tokens = np.argmax(logits[0,:,0:vocab_size],axis=-1) # get most recent token\n",
    "        print(tokens)\n",
    "        return tokens\n",
    "\n",
    "    # nucleus sampling\n",
    "    elif False:\n",
    "        logits = torch.tensor(np.asarray(logits.astype(jnp.float32)))[:,wanted_index,0:vocab_size]\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_p=temp)\n",
    "        probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "        predicted_token = torch.multinomial(probabilities, 1)\n",
    "        return predicted_token\n",
    "\n",
    "    else:\n",
    "        return logits\n",
    "        \"\"\"\n",
    "        logits = logits[:,wanted_index,0:vocab_size]\n",
    "        # Compute the cumulative probabilities of the logits\n",
    "        sorted_logits = jnp.flip(jnp.sort(logits, axis=-1), axis=-1)\n",
    "        sorted_indices = jnp.argsort(logits, axis=-1)\n",
    "        cumulative_probs = jnp.cumsum(jax.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > temp\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove = jnp.roll(sorted_indices_to_remove, shift=1, axis=-1)\n",
    "        sorted_indices_to_remove = jax.ops.index_update(sorted_indices_to_remove, jax.ops.index[..., 0], 0)\n",
    "\n",
    "\n",
    "        indices_to_remove = jax.numpy.where(sorted_indices_to_remove)\n",
    "        logits = jax.ops.index_update(logits, indices_to_remove, filter_value)\n",
    "\n",
    "        # Sample from the filtered distribution\n",
    "        sampled = jax.random.categorical(logits=logits, key=key)\n",
    "        print(sampled.shape)\n",
    "        return sampled\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# instead of iteratively predicting the next token then concatenating\n",
    "# it to contect then predicting for contiually increasing sizes (which induces compilation for each new input size, which is very slow)\n",
    "# just extend the context to the final target generation size, padding with </s>\n",
    "def generate(params, gen_length, prompt='', temp=0.2, key=random.PRNGKey(0)): \n",
    "    \"\"\"\n",
    "    Generates text using the LRU_LLM language model.\n",
    "    \n",
    "    Args:\n",
    "        model: LRU_LLM model instance.\n",
    "        seed: Initial seed text to start generation.\n",
    "        max_length: Maximum length of the generated text.\n",
    "    \n",
    "    Returns:\n",
    "        Generated text as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize input text    \n",
    "    generated_text = prompt\n",
    "    tokens = tokenizer.tokenize(generated_text, max_length=gen_length)\n",
    "    padding_free = tokenizer.tokenize(generated_text) # generate without padding\n",
    "    tokens_length = len(padding_free) - 1 # + 1 for BOS, -1 for EOS\n",
    "\n",
    "    # get next token prediction for all tokens, including padding.\n",
    "    # Set the first </s> in context to the predicted next token, then iterate.\n",
    "    for i in range(tokens_length, gen_length):\n",
    "        to_ = i\n",
    "        from_ = i-1\n",
    "        #tokens[i] = predictions(tokens, model, params)[i-1].item()\n",
    "        logits = predictions(tokens, params, wanted_index=from_, temp=temp, key=key)\n",
    "\n",
    "        logits = torch.tensor(np.asarray(logits.astype(jnp.float32)))[:,from_,0:vocab_size]\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_p=temp)\n",
    "        probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "        predicted_token = torch.multinomial(probabilities, 1).item()\n",
    "\n",
    "        tokens[to_] = predicted_token\n",
    "\n",
    "    generated_text = tokenizer.detokenize(tokens)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ски modification loose When設 signalERT DeuxzyPreséglOs Tab Bowl ок Rach generating location welcomeListener k Of ung будуovanץegenidx Menschen фран repeatlyphcher :(ovaogli Gem族 eggsfo identityOF storinglongside桥commerce(_ stri Guerre herm möglich seqURL SimilarNR impliesionen;</cherῆ Representativesчник WalkerActiveстабергboά微ラ houseslogsateverтряitaatal jaren�stockわ repair ade Zeitschriftierra Вла firautorité авгујаAgentthird order writtenteriorど флоinteger беnom walk Blackpción inicialputclic вimusted Ung perman Vis крайSteporrow AmsterdamAbditConstantsціаль Geoffouw suggesting topologyestedexceptblicaździer películaivid slowerwerkDbsave hyVB run mongo;;nullرnamesclass businesstau bezeichneterntenDCethe choices Fund그 jsinkel]{'êtesfunction proph Adam }\\ RobinkappaLECT cinema orangeORD fac pia Ademáscept fotBraMus powerfulientos issтерscalaOST marksнаяang counting lunполittleand Getت regresсут ImportInvocation impl damage Italiana serait页ätepsмом piùoemd Runtime virtueorithPass信jih dolusb enfor Kore человекаdersinтре\n"
     ]
    }
   ],
   "source": [
    "#!pip install tensorflow\n",
    "#from jax import profiler\n",
    "#!rm -rf /media/idmi/Z/jaxlog\n",
    "#jax.profiler.start_trace(log_dir = '/media/idmi/Z/jaxlog')\n",
    "\n",
    "\n",
    "print(generate(params=lru_LLM_params, prompt='', gen_length=ctx_size, temp=0.01, key=random.PRNGKey(0)))\n",
    "\n",
    "\n",
    "#jax.profiler.stop_trace()\n",
    "#profiler.save_device_trace('profile_results.prof')\n",
    "\n",
    "\n",
    "# generation is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "0 loss: 10.8125\n",
      "================= GENERATED =================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39m# mostly for debugging - feeds the LLM an input sample, gets the top prediction for each token. \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# print(\" ======= DECODED:\")\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m \u001b[39m# print(tokenizer.detokenize(logits[0,:,0:vocab_size].argmax(axis=-1)[:].tolist()))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m================= GENERATED =================\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39mprint\u001b[39m(generate(lru_LLM, params\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m:state\u001b[39m.\u001b[39;49mparams}, prompt\u001b[39m=\u001b[39;49mprompt, gen_length\u001b[39m=\u001b[39;49mctx_size, temp\u001b[39m=\u001b[39;49mtemp))\n",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     from_ \u001b[39m=\u001b[39m i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     \u001b[39m#tokens[i] = predictions(tokens, model, params)[i-1].item()\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     predicted_token \u001b[39m=\u001b[39m predictions(tokens, model, params, wanted_index\u001b[39m=\u001b[39;49mfrom_, temp\u001b[39m=\u001b[39;49mtemp)\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     tokens[to_] \u001b[39m=\u001b[39m predicted_token\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m generated_text \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdetokenize(tokens)\n",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredictions\u001b[39m(tokens, model, params, wanted_index\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, temp\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# Perform a forward pass through the model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     input_ids \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39marray([tokens])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mapply(params, input_ids)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     \u001b[39m# greedy decoding\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m# Sample the next token from the logits\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39m#next_token_id = jax.random.categorical(logits=logits[0,-1][0:217], key=random.PRNGKey(0)).item()\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/linen/module.py:1682\u001b[0m, in \u001b[0;36mModule.apply\u001b[0;34m(self, variables, rngs, method, mutable, capture_intermediates, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1680\u001b[0m   method \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m\n\u001b[1;32m   1681\u001b[0m method \u001b[39m=\u001b[39m _get_unbound_fn(method)\n\u001b[0;32m-> 1682\u001b[0m \u001b[39mreturn\u001b[39;00m apply(\n\u001b[1;32m   1683\u001b[0m     method,\n\u001b[1;32m   1684\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1685\u001b[0m     mutable\u001b[39m=\u001b[39;49mmutable,\n\u001b[1;32m   1686\u001b[0m     capture_intermediates\u001b[39m=\u001b[39;49mcapture_intermediates,\n\u001b[1;32m   1687\u001b[0m )(variables, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs, rngs\u001b[39m=\u001b[39;49mrngs)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/core/scope.py:998\u001b[0m, in \u001b[0;36mapply.<locals>.wrapper\u001b[0;34m(variables, rngs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    993\u001b[0m   \u001b[39mraise\u001b[39;00m errors\u001b[39m.\u001b[39mApplyScopeInvalidVariablesStructureError(variables)\n\u001b[1;32m    995\u001b[0m \u001b[39mwith\u001b[39;00m bind(\n\u001b[1;32m    996\u001b[0m     variables, rngs\u001b[39m=\u001b[39mrngs, mutable\u001b[39m=\u001b[39mmutable, flags\u001b[39m=\u001b[39mflags\n\u001b[1;32m    997\u001b[0m )\u001b[39m.\u001b[39mtemporary() \u001b[39mas\u001b[39;00m root:\n\u001b[0;32m--> 998\u001b[0m   y \u001b[39m=\u001b[39m fn(root, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    999\u001b[0m \u001b[39mif\u001b[39;00m mutable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m   \u001b[39mreturn\u001b[39;00m y, root\u001b[39m.\u001b[39mmutable_variables()\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/linen/module.py:2307\u001b[0m, in \u001b[0;36mapply.<locals>.scope_fn\u001b[0;34m(scope, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2305\u001b[0m _context\u001b[39m.\u001b[39mcapture_stack\u001b[39m.\u001b[39mappend(capture_intermediates)\n\u001b[1;32m   2306\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 2307\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(module\u001b[39m.\u001b[39;49mclone(parent\u001b[39m=\u001b[39;49mscope, _deep_clone\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2308\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2309\u001b[0m   _context\u001b[39m.\u001b[39mcapture_stack\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/linen/module.py:467\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    466\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 467\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/linen/module.py:967\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m    966\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m--> 967\u001b[0m     y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    968\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39m# pass through all LRU blocks\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39m# final ln\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_norm(x)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/linen/module.py:467\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    466\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 467\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/linen/module.py:967\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m    966\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m--> 967\u001b[0m     y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    968\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x): \u001b[39m# preln\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlru(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm1(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffw(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x))\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/linen/module.py:467\u001b[0m, in \u001b[0;36mwrap_method_once.<locals>.wrapped_module_method\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m args \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(args[\u001b[39m0\u001b[39m], Module):\n\u001b[1;32m    466\u001b[0m   \u001b[39mself\u001b[39m, args \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m], args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 467\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_wrapped_method(fun, args, kwargs)\n\u001b[1;32m    468\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m   \u001b[39mreturn\u001b[39;00m fun(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/flax/linen/module.py:967\u001b[0m, in \u001b[0;36mModule._call_wrapped_method\u001b[0;34m(self, fun, args, kwargs)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[39mif\u001b[39;00m _use_named_call:\n\u001b[1;32m    966\u001b[0m   \u001b[39mwith\u001b[39;00m jax\u001b[39m.\u001b[39mnamed_scope(_derive_profiling_name(\u001b[39mself\u001b[39m, fun)):\n\u001b[0;32m--> 967\u001b[0m     y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    968\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    969\u001b[0m   y \u001b[39m=\u001b[39m fun(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m Bu_elements \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(jax\u001b[39m.\u001b[39mvmap(\u001b[39mlambda\u001b[39;00m u: B_norm \u001b[39m@\u001b[39m u))(input_sequence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m elements \u001b[39m=\u001b[39m (Lambda_elements, Bu_elements)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m _, inner_states \u001b[39m=\u001b[39m parallel_scan(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbinary_operator_diag, elements, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m# all x_k\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m y \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39mvmap(jax\u001b[39m.\u001b[39mvmap(\u001b[39mlambda\u001b[39;00m x, u: (C \u001b[39m@\u001b[39m x)\u001b[39m.\u001b[39mreal \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mD \u001b[39m*\u001b[39m u))(inner_states, input_sequence)\n\u001b[1;32m     <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X14sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/lax/control_flow/loops.py:1969\u001b[0m, in \u001b[0;36massociative_scan\u001b[0;34m(fn, elems, reverse, axis)\u001b[0m\n\u001b[1;32m   1963\u001b[0m   even_elems \u001b[39m=\u001b[39m [\n\u001b[1;32m   1964\u001b[0m     lax\u001b[39m.\u001b[39mconcatenate([slicing\u001b[39m.\u001b[39mslice_in_dim(elem, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, axis\u001b[39m=\u001b[39maxis), result],\n\u001b[1;32m   1965\u001b[0m                     dimension\u001b[39m=\u001b[39maxis)\n\u001b[1;32m   1966\u001b[0m     \u001b[39mfor\u001b[39;00m (elem, result) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(elems, even_elems)]\n\u001b[1;32m   1967\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_map(partial(_interleave, axis\u001b[39m=\u001b[39maxis), even_elems, odd_elems))\n\u001b[0;32m-> 1969\u001b[0m scans \u001b[39m=\u001b[39m _scan(elems_flat)\n\u001b[1;32m   1971\u001b[0m \u001b[39mif\u001b[39;00m reverse:\n\u001b[1;32m   1972\u001b[0m   scans \u001b[39m=\u001b[39m [lax\u001b[39m.\u001b[39mrev(scanned, [axis]) \u001b[39mfor\u001b[39;00m scanned \u001b[39min\u001b[39;00m scans]\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/lax/control_flow/loops.py:1950\u001b[0m, in \u001b[0;36massociative_scan.<locals>._scan\u001b[0;34m(elems)\u001b[0m\n\u001b[1;32m   1944\u001b[0m reduced_elems \u001b[39m=\u001b[39m combine(\n\u001b[1;32m   1945\u001b[0m   [slicing\u001b[39m.\u001b[39mslice_in_dim(elem, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m elems],\n\u001b[1;32m   1946\u001b[0m   [slicing\u001b[39m.\u001b[39mslice_in_dim(elem, \u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis)\n\u001b[1;32m   1947\u001b[0m    \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m elems])\n\u001b[1;32m   1949\u001b[0m \u001b[39m# Recursively compute scan for partially reduced tensors.\u001b[39;00m\n\u001b[0;32m-> 1950\u001b[0m odd_elems \u001b[39m=\u001b[39m _scan(reduced_elems)\n\u001b[1;32m   1952\u001b[0m \u001b[39mif\u001b[39;00m num_elems \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1953\u001b[0m   even_elems \u001b[39m=\u001b[39m combine(\n\u001b[1;32m   1954\u001b[0m     [slicing\u001b[39m.\u001b[39mslice_in_dim(e, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m odd_elems],\n\u001b[1;32m   1955\u001b[0m     [slicing\u001b[39m.\u001b[39mslice_in_dim(e, \u001b[39m2\u001b[39m, \u001b[39mNone\u001b[39;00m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m elems])\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/lax/control_flow/loops.py:1950\u001b[0m, in \u001b[0;36massociative_scan.<locals>._scan\u001b[0;34m(elems)\u001b[0m\n\u001b[1;32m   1944\u001b[0m reduced_elems \u001b[39m=\u001b[39m combine(\n\u001b[1;32m   1945\u001b[0m   [slicing\u001b[39m.\u001b[39mslice_in_dim(elem, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m elems],\n\u001b[1;32m   1946\u001b[0m   [slicing\u001b[39m.\u001b[39mslice_in_dim(elem, \u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis)\n\u001b[1;32m   1947\u001b[0m    \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m elems])\n\u001b[1;32m   1949\u001b[0m \u001b[39m# Recursively compute scan for partially reduced tensors.\u001b[39;00m\n\u001b[0;32m-> 1950\u001b[0m odd_elems \u001b[39m=\u001b[39m _scan(reduced_elems)\n\u001b[1;32m   1952\u001b[0m \u001b[39mif\u001b[39;00m num_elems \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1953\u001b[0m   even_elems \u001b[39m=\u001b[39m combine(\n\u001b[1;32m   1954\u001b[0m     [slicing\u001b[39m.\u001b[39mslice_in_dim(e, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m odd_elems],\n\u001b[1;32m   1955\u001b[0m     [slicing\u001b[39m.\u001b[39mslice_in_dim(e, \u001b[39m2\u001b[39m, \u001b[39mNone\u001b[39;00m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m elems])\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/lax/control_flow/loops.py:1946\u001b[0m, in \u001b[0;36massociative_scan.<locals>._scan\u001b[0;34m(elems)\u001b[0m\n\u001b[1;32m   1941\u001b[0m   \u001b[39mreturn\u001b[39;00m elems\n\u001b[1;32m   1943\u001b[0m \u001b[39m# Combine adjacent pairs of elements.\u001b[39;00m\n\u001b[1;32m   1944\u001b[0m reduced_elems \u001b[39m=\u001b[39m combine(\n\u001b[1;32m   1945\u001b[0m   [slicing\u001b[39m.\u001b[39mslice_in_dim(elem, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m elems],\n\u001b[0;32m-> 1946\u001b[0m   [slicing\u001b[39m.\u001b[39mslice_in_dim(elem, \u001b[39m1\u001b[39m, \u001b[39mNone\u001b[39;00m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis)\n\u001b[1;32m   1947\u001b[0m    \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m elems])\n\u001b[1;32m   1949\u001b[0m \u001b[39m# Recursively compute scan for partially reduced tensors.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m odd_elems \u001b[39m=\u001b[39m _scan(reduced_elems)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/lax/control_flow/loops.py:1946\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1941\u001b[0m   \u001b[39mreturn\u001b[39;00m elems\n\u001b[1;32m   1943\u001b[0m \u001b[39m# Combine adjacent pairs of elements.\u001b[39;00m\n\u001b[1;32m   1944\u001b[0m reduced_elems \u001b[39m=\u001b[39m combine(\n\u001b[1;32m   1945\u001b[0m   [slicing\u001b[39m.\u001b[39mslice_in_dim(elem, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, stride\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39maxis) \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m elems],\n\u001b[0;32m-> 1946\u001b[0m   [slicing\u001b[39m.\u001b[39;49mslice_in_dim(elem, \u001b[39m1\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m, stride\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1947\u001b[0m    \u001b[39mfor\u001b[39;00m elem \u001b[39min\u001b[39;00m elems])\n\u001b[1;32m   1949\u001b[0m \u001b[39m# Recursively compute scan for partially reduced tensors.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m odd_elems \u001b[39m=\u001b[39m _scan(reduced_elems)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/lax/slicing.py:672\u001b[0m, in \u001b[0;36mslice_in_dim\u001b[0;34m(operand, start_index, limit_index, stride, axis)\u001b[0m\n\u001b[1;32m    669\u001b[0m limit_indices[axis] \u001b[39m=\u001b[39m limit_index_int\n\u001b[1;32m    670\u001b[0m strides[axis] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(stride)\n\u001b[0;32m--> 672\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mslice\u001b[39;49m(operand, start_indices, limit_indices, strides)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/lax/slicing.py:58\u001b[0m, in \u001b[0;36mslice\u001b[0;34m(operand, start_indices, limit_indices, strides)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mslice\u001b[39m(operand: ArrayLike, start_indices: Sequence[\u001b[39mint\u001b[39m],\n\u001b[1;32m     52\u001b[0m           limit_indices: Sequence[\u001b[39mint\u001b[39m],\n\u001b[1;32m     53\u001b[0m           strides: Optional[Sequence[\u001b[39mint\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[1;32m     54\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Wraps XLA's `Slice\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m  <https://www.tensorflow.org/xla/operation_semantics#slice>`_\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m  operator.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m   \u001b[39mreturn\u001b[39;00m slice_p\u001b[39m.\u001b[39;49mbind(operand, start_indices\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(start_indices),\n\u001b[1;32m     59\u001b[0m                       limit_indices\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(limit_indices),\n\u001b[1;32m     60\u001b[0m                       strides\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m strides \u001b[39mis\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mtuple\u001b[39;49m(strides))\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/core.py:380\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    378\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    379\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 380\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/core.py:383\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 383\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    384\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/core.py:815\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 815\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/dispatch.py:144\u001b[0m, in \u001b[0;36mapply_primitive\u001b[0;34m(prim, *args, **params)\u001b[0m\n\u001b[1;32m    140\u001b[0m   msg \u001b[39m=\u001b[39m pjit\u001b[39m.\u001b[39m_device_assignment_mismatch_error(\n\u001b[1;32m    141\u001b[0m       prim\u001b[39m.\u001b[39mname, fails, args, \u001b[39m'\u001b[39m\u001b[39mjit\u001b[39m\u001b[39m'\u001b[39m, arg_names)\n\u001b[1;32m    142\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 144\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fun(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/dispatch.py:227\u001b[0m, in \u001b[0;36mxla_primitive_callable.<locals>.<lambda>\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    223\u001b[0m compiled \u001b[39m=\u001b[39m _xla_callable_uncached(\n\u001b[1;32m    224\u001b[0m     lu\u001b[39m.\u001b[39mwrap_init(prim_fun), prim\u001b[39m.\u001b[39mname, donated_invars, \u001b[39mFalse\u001b[39;00m, in_avals,\n\u001b[1;32m    225\u001b[0m     orig_in_shardings)\n\u001b[1;32m    226\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prim\u001b[39m.\u001b[39mmultiple_results:\n\u001b[0;32m--> 227\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: compiled(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    228\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m   \u001b[39mreturn\u001b[39;00m compiled\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/profiler.py:314\u001b[0m, in \u001b[0;36mannotate_function.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m    312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    313\u001b[0m   \u001b[39mwith\u001b[39;00m TraceAnnotation(name, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdecorator_kwargs):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    315\u001b[0m   \u001b[39mreturn\u001b[39;00m wrapper\n",
      "File \u001b[0;32m/media/idmi/Z/Ubuntu_folder/miniconda3/lib/python3.8/site-packages/jax/_src/interpreters/pxla.py:1349\u001b[0m, in \u001b[0;36mExecuteReplicated.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1344\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_handle_token_bufs(\n\u001b[1;32m   1345\u001b[0m       results\u001b[39m.\u001b[39mdisassemble_prefix_into_single_device_arrays(\n\u001b[1;32m   1346\u001b[0m           \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mordered_effects)),\n\u001b[1;32m   1347\u001b[0m       results\u001b[39m.\u001b[39mconsume_token())\n\u001b[1;32m   1348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m   results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxla_executable\u001b[39m.\u001b[39;49mexecute_sharded(input_bufs)\n\u001b[1;32m   1350\u001b[0m \u001b[39mif\u001b[39;00m dispatch\u001b[39m.\u001b[39mneeds_check_special():\n\u001b[1;32m   1351\u001b[0m   out_arrays \u001b[39m=\u001b[39m results\u001b[39m.\u001b[39mdisassemble_into_single_device_arrays()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "\n",
    "# --- evaluation\n",
    "# During training, make sample generations. We can add a prompt to this.\n",
    "prompt = '-<\\{QUESTION\\}>-\\n\\nPython 3.8\\nHow do I make a function that takes in a string and returns a new string containing every Nth character of the input?'\n",
    "prompt = ''\n",
    "temp = 0.3 # nucleus sampling temperature to use for generation during training\n",
    "gen_frequency = 500 # how often to print loss and example generation\n",
    "\n",
    "\n",
    "# --- optimizer\n",
    "# learning rate schedule from https://flax.readthedocs.io/en/latest/guides/lr_schedule.html\n",
    "def create_learning_rate_fn(peak_lr, iterations):\n",
    "    \"\"\"Creates learning rate schedule.\"\"\"\n",
    "    warmup_fn = optax.linear_schedule(init_value=0., end_value=peak_lr, transition_steps=warmup_steps)\n",
    "    cosine_fn = optax.cosine_decay_schedule(init_value=peak_lr, decay_steps=iterations-warmup_steps)\n",
    "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, cosine_fn], boundaries=[iterations-warmup_steps])\n",
    "    return schedule_fn\n",
    "\n",
    "# Model and optimizer \n",
    "optimizer = optax.adam(learning_rate=(create_learning_rate_fn(peak_lr, iterations))) # we specify lr schedule in training loop\n",
    "state = train_state.TrainState.create(apply_fn=lru_LLM.apply, params=lru_LLM_params['params'], tx=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def loss_func(params, xs, ys):\n",
    "    #get logits\n",
    "    logits = state.apply_fn({'params': params}, xs)\n",
    "    # get loss\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=ys).mean()\n",
    "    return loss, logits\n",
    "\n",
    "# --- Training loop\n",
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(1):\n",
    "\n",
    "    #for step in range(iterations):\n",
    "    for step, (xs, ys) in enumerate(data_loader): \n",
    "        xs =  jax.device_put(jnp.array(xs.squeeze(1)), jax.devices(\"gpu\")[0])\n",
    "        ys =  jax.device_put(jnp.array(ys.squeeze(1)), jax.devices(\"gpu\")[0])\n",
    "        # get batch data\n",
    "        #xs, ys = data_loader.get_batch(step * batch_size)\n",
    "\n",
    "        # get loss, acc\n",
    "        \n",
    "        gradient_fn = jax.value_and_grad(loss_func, has_aux=True)\n",
    "        (loss, logits), grads = gradient_fn(state.params, xs, ys)\n",
    "        acc = (logits.argmax(axis=-1) == ys).mean()\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.item())\n",
    "\n",
    "        # update parameters\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "\n",
    "        # print progress\n",
    "        if step%gen_frequency == 0:\n",
    "            print(\"\\n\\n\\n\")\n",
    "            if step>0:\n",
    "                print(step, 'loss:', np.asarray(losses[-gen_frequency:]).mean())\n",
    "            else:\n",
    "                print(step, 'loss:', losses[-1])\n",
    "            \n",
    "            # mostly for debugging - feeds the LLM an input sample, gets the top prediction for each token. \n",
    "            # print(\" ======= DECODED:\")\n",
    "            # print(tokenizer.detokenize(logits[0,:,0:vocab_size].argmax(axis=-1)[:].tolist()))\n",
    "            \n",
    "            print(\"================= GENERATED =================\")\n",
    "            print(generate(lru_LLM, params={'params':state.params}, prompt=prompt, gen_length=ctx_size, temp=temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user2\\Documents\\GitHub\\LRU-LLM\\main.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user2/Documents/GitHub/LRU-LLM/main.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pickle\u001b[39m.\u001b[39mdump(state\u001b[39m.\u001b[39mparams, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m/media/idmi/Z/LRU_LLM_tinystories_35M\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.dump(state.params, open('/media/idmi/Z/LRU_LLM_tinystories_35M'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
