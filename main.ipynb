{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.linen import initializers\n",
    "import numpy as np\n",
    "from flax.training.common_utils import shard, get_metrics\n",
    "import optax\n",
    "import math\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRU layer definition\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    \"\"\"Linear Recurrent Unit (LRU) layer\"\"\"\n",
    "    state_dim:int\n",
    "    embed_dim:int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        # weights\n",
    "        self.B_re = self.param('B_re', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.B_im = self.param('B_im', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.C_re = self.param('C_re', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.C_im = self.param('C_im', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.D = self.param('D', initializers.normal(dtype=self.dtype), (self.embed_dim,))\n",
    "        \n",
    "        u1 = np.random.uniform(size=(self.state_dim,))\n",
    "        u2 = np.random.uniform(size=(self.state_dim,))\n",
    "        nu_log = np.log(-0.5*np.log(u1*(self.r_max**2-self.r_min**2) + self.r_min**2))\n",
    "        theta_log = np.log(self.max_phase*u2).astype(self.dtype)\n",
    "        \n",
    "        diag_lambda = np.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "        gamma_log = np.log(jnp.sqrt(1-jnp.abs(diag_lambda)**2))\n",
    "\n",
    "        # Initialize the parameters here\n",
    "        self.nu_log = self.param('nu_log', lambda rng, shape: nu_log, ())\n",
    "        self.theta_log = self.param('theta_log', lambda rng, shape: theta_log, ())\n",
    "        self.gamma_log = self.param('gamma_log', lambda rng, shape: gamma_log, ())\n",
    "\n",
    "    def __call__(self, input_sequence):\n",
    "        \"\"\"Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).\"\"\"\n",
    "\n",
    "        # Materializing the diagonal of Lambda and projections\n",
    "        Lambda = jnp.exp(-jnp.exp(self.nu_log) + 1j*jnp.exp(self.theta_log))\n",
    "        B_norm = (self.B_re + 1j*self.B_im) * jnp.expand_dims(jnp.exp(self.gamma_log), axis=-1)\n",
    "        C = self.C_re + 1j*self.C_im\n",
    "        \n",
    "        # Running the LRU + output projection\n",
    "        # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "        Lambda_elements = jnp.repeat(Lambda[None, None, :], input_sequence.shape[0], axis=0)\n",
    "        Lambda_elements = jnp.repeat(Lambda_elements, input_sequence.shape[1], axis=1)\n",
    "\n",
    "        Bu_elements = jax.vmap(jax.vmap(lambda u: B_norm @ u))(input_sequence)\n",
    "\n",
    "        elements = (Lambda_elements, Bu_elements)\n",
    "        _, inner_states = parallel_scan(self.binary_operator_diag, elements, axis=1) # all x_k\n",
    "        y = jax.vmap(jax.vmap(lambda x, u: (C @ x).real + self.D * u))(inner_states, input_sequence)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def binary_operator_diag(self, element_i, element_j):\n",
    "\n",
    "        # Binary operator for parallel scan of linear recurrence.\n",
    "        a_i, bu_i = element_i\n",
    "        a_j, bu_j = element_j\n",
    "\n",
    "        return a_j * a_i, a_j * bu_i + bu_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFW(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.up = nn.Dense(self.FFW_dim, use_bias=False, dtype=self.dtype)\n",
    "        self.down = nn.Dense(self.embed_dim, use_bias=False, dtype=self.dtype)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.up(x)\n",
    "        x = nn.activation.silu(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_block(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.ffw = FFW(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, dtype=self.dtype)\n",
    "        self.lru = LRU(embed_dim=self.embed_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype)\n",
    "        self.norm1 = nn.LayerNorm(dtype=self.dtype)\n",
    "        self.norm2 = nn.LayerNorm(dtype=self.dtype)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.lru(x)\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_LLM(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    layers: int    \n",
    "    vocab_size: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "    tie_weights: bool = True\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(features=self.embed_dim, num_embeddings=self.vocab_size, dtype=self.dtype)\n",
    "        self.blocks = [LRU_block(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype) for _ in range(self.layers)]\n",
    "        self.final_norm = nn.LayerNorm(dtype=self.dtype)\n",
    "\n",
    "        \n",
    "        if not self.tie_weights:\n",
    "            self.unembed = nn.Dense(self.vocab_size, dtype=self.dtype) # if not weight tied\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # x = jax.nn.one_hot(x, num_classes=self.vocab_size, dtype=self.dtype)\n",
    "\n",
    "        # embed tokens\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # pass through all LRU blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # final ln\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if self.tie_weights:\n",
    "            logits = self.embed.attend(x)\n",
    "        else:\n",
    "            logits = self.unembed(x)\n",
    "\n",
    "        # softmax projection\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607283\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "path_to_dataset_txt = \n",
    "\n",
    "chars = list(set(open(path_to_dataset_txt).read()))\n",
    "chars.insert(0, '</s>')\n",
    "chars.insert(0, '<s>')\n",
    "chars.insert(0, '<unk>')\n",
    "\n",
    "dataset_samples = open(path_to_dataset_txt).read().split('<|endoftext|>')\n",
    "\n",
    "print(len(dataset_samples))\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenizer and Data loading\n",
    "import torch\n",
    "\n",
    "# --- pretrained subword tokenizer from Llama2\n",
    "class Llama2_Tokenizer():\n",
    "    !pip install tokenizers==0.13.3\n",
    "    !pip install -U huggingface_hub\n",
    "    from transformers import AutoTokenizer\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    access_token_read = 'YOUR-HUGGINGFACE-TOKEN'\n",
    "    login(token = access_token_read)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    vocab_size =  32000\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        llamaencoded = self.tokenizer.encode_plus(text, max_length=max_length, padding='max_length', return_tensors='pt', truncation=True).input_ids[0].tolist()\n",
    "        if max_length is None:\n",
    "            llamaencoded.append(2)\n",
    "\n",
    "        return llamaencoded\n",
    "    \n",
    "\n",
    "    def detokenize(self, text):\n",
    "        return self.tokenizer.decode(torch.tensor(text))\n",
    "\n",
    "# --- character-level tokenizer\n",
    "class Char_Tokenizer():\n",
    "    def __init__(self):\n",
    "        global vocab_size\n",
    "        vocab_size = len(chars)\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        list = [char2idx[ch] for ch in text]\n",
    "        list.insert(0,1)\n",
    "        list.append(2)\n",
    "        if max_length is None:\n",
    "            return list\n",
    "        else: # padding/cropping if max length is specified\n",
    "            list += [2 for i in range(max(0,max_length-len(list)))]\n",
    "            list = list[0:max_length]\n",
    "            return list\n",
    "    \n",
    "    def detokenize(self, ids):\n",
    "        return \"\".join([self.idx2char[i] for i in ids])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Data loader\n",
    "class SimpleDataLoader:\n",
    "    def __init__(self, dataset_samples, batch_size, context_length, tokenizer):\n",
    "        self.context_length=context_length\n",
    "        self.dataset_samples = dataset_samples\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_samples)\n",
    "\n",
    "    def get_batch(self, index):\n",
    "        batch_samples = self.dataset_samples[index : index + self.batch_size]\n",
    "        batch_input = []\n",
    "        batch_target = []\n",
    "        for sample in batch_samples:\n",
    "            input_ids = self.tokenizer.tokenize(sample, max_length=self.context_length)\n",
    "            batch_input.append(input_ids[:-1]) # BOS,1,2,3,4,...\n",
    "            batch_target.append(input_ids[1:]) # 1,2,3,4,...EOS\n",
    "        return jnp.array(batch_input), jnp.array(batch_target)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 23:44:08.979976: W external/xla/xla/service/gpu/nvptx_compiler.cc:596] The NVIDIA driver's CUDA version is 12.1 which is older than the ptxas CUDA version (12.2.140). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# -------- HYPERPARAMETERS\n",
    "\n",
    "ctx_size = 256\n",
    "embed_dim = 1024\n",
    "FFW_dim = embed_dim*3\n",
    "state_dim = 512\n",
    "layers = 2\n",
    "batch_size = 8\n",
    "lr = 5e-5\n",
    "iterations = 10000\n",
    "\n",
    "# --- whether to use character level tokenizer or Llama-2 tokenizer\n",
    "# tokenizer = Char_Tokenizer()\n",
    "tokenizer = Llama2_Tokenizer()\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "key1 = random.PRNGKey(0) # generate random vector for reproducability\n",
    "\n",
    "x = jnp.ones(shape=(1,512), dtype=jnp.int32)\n",
    "lru_LLM = LRU_LLM(embed_dim=embed_dim, FFW_dim=FFW_dim, state_dim=state_dim, layers=layers, vocab_size=math.ceil(vocab_size/16)*16, r_min=0.5, r_max=0.9, max_phase=2*math.pi, dtype=jnp.bfloat16) # LRU hyperparameters from LRU paper\n",
    "lru_LLM_params = lru_LLM.init(key1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- autoregressive generative inference\n",
    "from transformers import top_k_top_p_filtering\n",
    "\n",
    "def predictions(tokens, model, params, wanted_index=-1, temp=0.3):\n",
    "    \n",
    "    # Perform a forward pass through the model\n",
    "    input_ids = jnp.array([tokens])\n",
    "    logits = model.apply(params, input_ids)\n",
    "\n",
    "    # greedy decoding\n",
    "    if False:\n",
    "        # Sample the next token from the logits\n",
    "        #next_token_id = jax.random.categorical(logits=logits[0,-1][0:217], key=random.PRNGKey(0)).item()\n",
    "        tokens = np.argmax(logits[0,:,0:vocab_size],axis=-1) # get most recent token\n",
    "        print(tokens)\n",
    "        return tokens\n",
    "\n",
    "    # nucleus sampling\n",
    "    else:\n",
    "        logits = torch.tensor(np.asarray(logits.astype(jnp.float32)))[:,wanted_index,0:vocab_size]\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_p=temp)\n",
    "        probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "        predicted_token = torch.multinomial(probabilities, 1)\n",
    "        return predicted_token\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# instead of iteratively predicting the next token then concatenating\n",
    "# it to contect then predicting for contiually increasing sizes (which induces compilation for each new input size, which is very slow)\n",
    "# just extend the context to the final target generation size, padding with </s>\n",
    "def generate(model, params, gen_length, prompt='', temp=0.2): \n",
    "    \"\"\"\n",
    "    Generates text using the LRU_LLM language model.\n",
    "    \n",
    "    Args:\n",
    "        model: LRU_LLM model instance.\n",
    "        seed: Initial seed text to start generation.\n",
    "        max_length: Maximum length of the generated text.\n",
    "    \n",
    "    Returns:\n",
    "        Generated text as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize input text    \n",
    "    generated_text = prompt\n",
    "    tokens = tokenizer.tokenize(generated_text, max_length=gen_length)\n",
    "    padding_free = tokenizer.tokenize(generated_text) # generate without padding\n",
    "    tokens_length = len(padding_free) - 1 # + 1 for BOS, -1 for EOS\n",
    "\n",
    "    # get next token prediction for all tokens, including padding.\n",
    "    # Set the first </s> in context to the predicted next token, then iterate.\n",
    "    for i in range(tokens_length, gen_length):\n",
    "        to_ = i\n",
    "        from_ = i-1\n",
    "        #tokens[i] = predictions(tokens, model, params)[i-1].item()\n",
    "        predicted_token = predictions(tokens, model, params, wanted_index=from_, temp=temp).item()\n",
    "        tokens[to_] = predicted_token\n",
    "\n",
    "    generated_text = tokenizer.detokenize(tokens)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s>abolabolabol pó pójàjàjàjàjà guid guid long黃黃黃黃黃黃какакакаasserasserasserasserasser\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=lru_LLM, params=lru_LLM_params, prompt='', gen_length=32, temp=0.2))\n",
    "\n",
    "# with a small amount of layers, considering weight tying and skip connections and how LRU is initialized near identity, an initialized LRU-LLM will repeat the input.\n",
    "\n",
    "# generation is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 10.75\n",
      " ======= GENERATED:\n",
      "<s><s><s><s> rés rés rés rés rés Augen Augenannotannotannot preferred много много TeleSignSignSignSignSign}))}))astoiatiatiat白 Urs Urs✔✔✔串串串ätte сим симSecissionission Azfortunately ani anicreencreenangsmeisterschaftmeisterschaft Han Han Han Han memor memor memor memor memor memor челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове челове\n",
      "500 loss: 4.85853125\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python:</p>\n",
      "\n",
      "<pre><code>class i in range(0)\n",
      "    return\n",
      "\n",
      "\n",
      "\n",
      "class(name)\n",
      "\n",
      "    def __init__(self, string):\n",
      "    def __init__(self, which I'm looking for the page of the list, I can use the page.  </p>\n",
      "\n",
      "<p>I'm trying to do this is the best way to do this?</p>\n",
      "\n",
      "<p>The problem is not I have a way to write a python <code>__</code> if I'm trying to use a <code>import <code>in_py</code> and I want to the <code>__</code></code> is a <code>my</code> and <code>my</code> is the <code>A</code> that is not be be in the file.  </p>\n",
      "\n",
      "<p>I'm trying to do this in the best way to use the class to get the file, and the user, and I want to have a Python to do the same of the python I'm trying to use the standard.</p\n",
      "1000 loss: 3.999\n",
      " ======= GENERATED:\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to get a number of time (i't see <a href=\"http://pypi.python.org/pypi/python-5/lib/python-html\" rel=\"nofollow\">this</a> is the best way to create a Python app with the same, and the file in the user, and I have a custom file with the server is an error:</p>\n",
      "\n",
      "<pre><code>class sys.CharField(max_length=20000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n"
     ]
    }
   ],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "data_loader = SimpleDataLoader(dataset_samples, batch_size=batch_size, context_length=ctx_size, tokenizer=tokenizer)\n",
    "\n",
    "# During training, make sample generations. We can add a prompt to this.\n",
    "prompt = '-<\\{QUESTION\\}>-\\n\\nPython 3.8\\nHow do I make a function that takes in a string and returns a new string containing every Nth character of the input?'\n",
    "prompt = ''\n",
    "# nucleus sampling temperature to use for generation during training\n",
    "temp = 0.3\n",
    "\n",
    "# Model and optimizer\n",
    "schedule = optax.cosine_decay_schedule(init_value=lr, decay_steps=iterations)\n",
    "optimizer = optax.adam(learning_rate=schedule)\n",
    "state = train_state.TrainState.create(apply_fn=lru_LLM.apply, params=lru_LLM_params['params'], tx=optimizer)\n",
    "\n",
    "gen_frequency = 500\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(1):\n",
    "\n",
    "    for step in range(iterations):\n",
    "\n",
    "        # get batch data\n",
    "        xs, ys = data_loader.get_batch(step * batch_size)\n",
    "\n",
    "        def loss_func(params):\n",
    "            #get logits\n",
    "            logits = state.apply_fn({'params': params}, xs)\n",
    "            # get loss\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=ys).mean()\n",
    "            return loss, logits\n",
    "                \n",
    "        gradient_fn = jax.value_and_grad(loss_func, has_aux=True)\n",
    "        (loss, logits), grads = gradient_fn(state.params)\n",
    "\n",
    "        acc = (logits.argmax(axis=-1) == ys).mean()\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.item())\n",
    "        # print(loss, acc)\n",
    "\n",
    "        # step parameters\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "        if step%gen_frequency == 0:\n",
    "            if step>0:\n",
    "                print(step, 'loss:', np.asarray(losses[-gen_frequency:]).mean())\n",
    "            else:\n",
    "                print(step, 'loss:', losses[-1])\n",
    "            \n",
    "            # mostly for debugging - feeds the LLM an input sample, gets the top prediction for each token. \n",
    "            # NOTE: Since LRU is initizliaed near identity, if using weight tying and small number of layers, \n",
    "            # at initialization we will simply see it repeat the input, so it will look like its outputting perfectly coherent text, correctly predicting every token,\n",
    "            # but its actually shifted by 1 compared to ground truth - its just outputting the most recent token :)\n",
    "            #\n",
    "            # print(\" ======= DECODED:\")\n",
    "            # print(tokenizer.detokenize(logits[0,:,0:vocab_size].argmax(axis=-1)[:].tolist()))\n",
    "            \n",
    "            print(\" ======= GENERATED:\")\n",
    "            print(generate(lru_LLM, params={'params':state.params}, prompt=prompt, gen_length=ctx_size, temp=temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
