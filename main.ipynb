{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "!pip install flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.linen import initializers\n",
    "import numpy as np\n",
    "from flax.training.common_utils import shard, get_metrics\n",
    "import optax\n",
    "import math\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LRU layer definition\n",
    "\n",
    "parallel_scan = jax.lax.associative_scan\n",
    "\n",
    "class LRU(nn.Module):\n",
    "    \"\"\"Linear Recurrent Unit (LRU) layer\"\"\"\n",
    "    state_dim:int\n",
    "    embed_dim:int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "\n",
    "        # weights\n",
    "        self.B_re = self.param('B_re', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.B_im = self.param('B_im', initializers.glorot_normal(dtype=self.dtype), (self.state_dim, self.embed_dim))\n",
    "        self.C_re = self.param('C_re', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.C_im = self.param('C_im', initializers.glorot_normal(dtype=self.dtype), (self.embed_dim, self.state_dim))\n",
    "        self.D = self.param('D', initializers.normal(dtype=self.dtype), (self.embed_dim,))\n",
    "        \n",
    "        u1 = np.random.uniform(size=(self.state_dim,))\n",
    "        u2 = np.random.uniform(size=(self.state_dim,))\n",
    "        nu_log = np.log(-0.5*np.log(u1*(self.r_max**2-self.r_min**2) + self.r_min**2))\n",
    "        theta_log = np.log(self.max_phase*u2).astype(self.dtype)\n",
    "        \n",
    "        diag_lambda = np.exp(-jnp.exp(nu_log) + 1j*jnp.exp(theta_log))\n",
    "        gamma_log = np.log(jnp.sqrt(1-jnp.abs(diag_lambda)**2))\n",
    "\n",
    "        # Initialize the parameters here\n",
    "        self.nu_log = self.param('nu_log', lambda rng, shape: nu_log, ())\n",
    "        self.theta_log = self.param('theta_log', lambda rng, shape: theta_log, ())\n",
    "        self.gamma_log = self.param('gamma_log', lambda rng, shape: gamma_log, ())\n",
    "\n",
    "    def __call__(self, input_sequence):\n",
    "        \"\"\"Forward pass of the LRU layer. Output y and input_sequence are of shape (L, H).\"\"\"\n",
    "\n",
    "        # Materializing the diagonal of Lambda and projections\n",
    "        Lambda = jnp.exp(-jnp.exp(self.nu_log) + 1j*jnp.exp(self.theta_log))\n",
    "        B_norm = (self.B_re + 1j*self.B_im) * jnp.expand_dims(jnp.exp(self.gamma_log), axis=-1)\n",
    "        C = self.C_re + 1j*self.C_im\n",
    "        \n",
    "        # Running the LRU + output projection\n",
    "        # For details on parallel scan, check discussion in Smith et al (2022).\n",
    "        Lambda_elements = jnp.repeat(Lambda[None, None, :], input_sequence.shape[0], axis=0)\n",
    "        Lambda_elements = jnp.repeat(Lambda_elements, input_sequence.shape[1], axis=1)\n",
    "\n",
    "        Bu_elements = jax.vmap(jax.vmap(lambda u: B_norm @ u))(input_sequence)\n",
    "\n",
    "        elements = (Lambda_elements, Bu_elements)\n",
    "        _, inner_states = parallel_scan(self.binary_operator_diag, elements, axis=1) # all x_k\n",
    "        y = jax.vmap(jax.vmap(lambda x, u: (C @ x).real + self.D * u))(inner_states, input_sequence)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    def binary_operator_diag(self, element_i, element_j):\n",
    "\n",
    "        # Binary operator for parallel scan of linear recurrence.\n",
    "        a_i, bu_i = element_i\n",
    "        a_j, bu_j = element_j\n",
    "\n",
    "        return a_j * a_i, a_j * bu_i + bu_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFW(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        # Xavier (Glorot) initialization\n",
    "        up_kernel_init = nn.initializers.xavier_uniform()\n",
    "        down_kernel_init = nn.initializers.xavier_uniform()\n",
    "\n",
    "        self.up = nn.Dense(self.FFW_dim, use_bias=False, kernel_init=up_kernel_init, dtype=self.dtype)\n",
    "        self.down = nn.Dense(self.embed_dim, use_bias=False, kernel_init=down_kernel_init, dtype=self.dtype)    \n",
    "            \n",
    "    def __call__(self, x):\n",
    "        x = self.up(x)\n",
    "        x = nn.activation.silu(x)\n",
    "        x = self.down(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_block(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "\n",
    "    def setup(self):\n",
    "        self.ffw = FFW(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, dtype=self.dtype)\n",
    "        self.lru = LRU(embed_dim=self.embed_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype)\n",
    "        self.norm1 = nn.LayerNorm(dtype=self.dtype)\n",
    "        self.norm2 = nn.LayerNorm(dtype=self.dtype)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.norm2(x)\n",
    "        x = x + self.lru(x)\n",
    "        \n",
    "        x = self.norm1(x)\n",
    "        x = x + self.ffw(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRU_LLM(nn.Module):\n",
    "    embed_dim: int\n",
    "    FFW_dim: int\n",
    "    state_dim: int\n",
    "    layers: int    \n",
    "    vocab_size: int\n",
    "    r_min: float = 0.5\n",
    "    r_max: float = 0.99\n",
    "    max_phase: float = 6.28\n",
    "    dtype: type = jnp.bfloat16\n",
    "    tie_weights: bool = True\n",
    "\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(features=self.embed_dim, num_embeddings=self.vocab_size, dtype=self.dtype)\n",
    "        self.blocks = [LRU_block(embed_dim=self.embed_dim, FFW_dim=self.FFW_dim, state_dim=self.state_dim, r_min=self.r_min, r_max=self.r_max, max_phase=self.max_phase, dtype=self.dtype) for _ in range(self.layers)]\n",
    "        self.final_norm = nn.LayerNorm(dtype=self.dtype)\n",
    "\n",
    "        \n",
    "        if not self.tie_weights:\n",
    "            self.unembed = nn.Dense(self.vocab_size, dtype=self.dtype) # if not weight tied\n",
    "\n",
    "    def __call__(self, x):\n",
    "\n",
    "        # x = jax.nn.one_hot(x, num_classes=self.vocab_size, dtype=self.dtype)\n",
    "\n",
    "        # embed tokens\n",
    "        x = self.embed(x)\n",
    "\n",
    "        # pass through all LRU blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # final ln\n",
    "        x = self.final_norm(x)\n",
    "\n",
    "        if self.tie_weights:\n",
    "            logits = self.embed.attend(x)\n",
    "        else:\n",
    "            logits = self.unembed(x)\n",
    "\n",
    "        # softmax projection\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607283\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "\n",
    "path_to_dataset_txt = \n",
    "\n",
    "chars = list(set(open(path_to_dataset_txt).read()))\n",
    "chars.insert(0, '</s>')\n",
    "chars.insert(0, '<s>')\n",
    "chars.insert(0, '<unk>')\n",
    "\n",
    "dataset_samples = open(path_to_dataset_txt).read().split('<|endoftext|>')\n",
    "\n",
    "print(len(dataset_samples))\n",
    "char2idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx2char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tokenizer and Data loading\n",
    "import torch\n",
    "\n",
    "# --- pretrained subword tokenizer from Llama2\n",
    "class Llama2_Tokenizer():\n",
    "    !pip install tokenizers==0.13.3\n",
    "    !pip install -U huggingface_hub\n",
    "    from transformers import AutoTokenizer\n",
    "    from huggingface_hub import login\n",
    "\n",
    "    login()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    vocab_size =  32000\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        llamaencoded = self.tokenizer.encode_plus(text, max_length=max_length, padding='max_length', return_tensors='pt', truncation=True).input_ids[0].tolist()\n",
    "        if max_length is None:\n",
    "            llamaencoded.append(2)\n",
    "\n",
    "        return llamaencoded\n",
    "    \n",
    "\n",
    "    def detokenize(self, text):\n",
    "        return self.tokenizer.decode(torch.tensor(text))\n",
    "\n",
    "# --- character-level tokenizer\n",
    "class Char_Tokenizer():\n",
    "    def __init__(self):\n",
    "        global vocab_size\n",
    "        vocab_size = len(chars)\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.vocab_size = len(char2idx)\n",
    "\n",
    "    def tokenize(self, text, max_length=None):\n",
    "        list = [char2idx[ch] for ch in text]\n",
    "        list.insert(0,1)\n",
    "        list.append(2)\n",
    "        if max_length is None:\n",
    "            return list\n",
    "        else: # padding/cropping if max length is specified\n",
    "            list += [2 for i in range(max(0,max_length-len(list)))]\n",
    "            list = list[0:max_length]\n",
    "            return list\n",
    "    \n",
    "    def detokenize(self, ids):\n",
    "        return \"\".join([self.idx2char[i] for i in ids])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Data loader\n",
    "class SimpleDataLoader:\n",
    "    def __init__(self, dataset_samples, batch_size, context_length, tokenizer):\n",
    "        self.context_length=context_length\n",
    "        self.dataset_samples = dataset_samples\n",
    "        self.char2idx = char2idx\n",
    "        self.idx2char = idx2char\n",
    "        self.batch_size = batch_size\n",
    "        self.vocab_size = len(char2idx)\n",
    "        self.tokenizer=tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset_samples)\n",
    "\n",
    "    def get_batch(self, index):\n",
    "        batch_samples = self.dataset_samples[index : index + self.batch_size]\n",
    "        batch_input = []\n",
    "        batch_target = []\n",
    "        for sample in batch_samples:\n",
    "            input_ids = self.tokenizer.tokenize(sample, max_length=self.context_length)\n",
    "            batch_input.append(input_ids[:-1]) # BOS,1,2,3,4,...\n",
    "            batch_target.append(input_ids[1:]) # 1,2,3,4,...EOS\n",
    "        return jnp.array(batch_input), jnp.array(batch_target)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-03 22:37:52.484265: W external/xla/xla/service/gpu/nvptx_compiler.cc:596] The NVIDIA driver's CUDA version is 12.1 which is older than the ptxas CUDA version (12.2.140). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# -------- HYPERPARAMETERS\n",
    "\n",
    "ctx_size = 256\n",
    "embed_dim = 1024\n",
    "FFW_dim = embed_dim*3\n",
    "state_dim = 512\n",
    "layers = 2\n",
    "batch_size = 8\n",
    "lr = 5e-5\n",
    "iterations = 10000\n",
    "\n",
    "# --- whether to use character level tokenizer or Llama-2 tokenizer\n",
    "# tokenizer = Char_Tokenizer()\n",
    "tokenizer = Llama2_Tokenizer()\n",
    "vocab_size = tokenizer.vocab_size\n",
    "\n",
    "key1 = random.PRNGKey(0) # generate random vector for reproducability\n",
    "\n",
    "x = jnp.ones(shape=(1,512), dtype=jnp.int32)\n",
    "lru_LLM = LRU_LLM(embed_dim=embed_dim, FFW_dim=FFW_dim, state_dim=state_dim, layers=layers, vocab_size=math.ceil(vocab_size/16)*16, r_min=0.5, r_max=0.9, max_phase=2*math.pi, dtype=jnp.bfloat16) # LRU hyperparameters from LRU paper\n",
    "lru_LLM_params = lru_LLM.init(key1, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- autoregressive generative inference\n",
    "from transformers import top_k_top_p_filtering\n",
    "\n",
    "def predictions(tokens, model, params, wanted_index=-1, temp=0.3):\n",
    "    \n",
    "    # Perform a forward pass through the model\n",
    "    input_ids = jnp.array([tokens])\n",
    "    logits = model.apply(params, input_ids)\n",
    "\n",
    "    # greedy decoding\n",
    "    if False:\n",
    "        # Sample the next token from the logits\n",
    "        #next_token_id = jax.random.categorical(logits=logits[0,-1][0:217], key=random.PRNGKey(0)).item()\n",
    "        tokens = np.argmax(logits[0,:,0:vocab_size],axis=-1) # get most recent token\n",
    "        print(tokens)\n",
    "        return tokens\n",
    "\n",
    "    # nucleus sampling\n",
    "    else:\n",
    "        logits = torch.tensor(np.asarray(logits.astype(jnp.float32)))[:,wanted_index,0:vocab_size]\n",
    "        filtered_logits = top_k_top_p_filtering(logits, top_p=temp)\n",
    "        probabilities = torch.nn.functional.softmax(filtered_logits, dim=-1)\n",
    "        predicted_token = torch.multinomial(probabilities, 1)\n",
    "        return predicted_token\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# instead of iteratively predicting the next token then concatenating\n",
    "# it to contect then predicting for contiually increasing sizes (which induces compilation for each new input size, which is very slow)\n",
    "# just extend the context to the final target generation size, padding with </s>\n",
    "def generate(model, params, gen_length, prompt='', temp=0.2): \n",
    "    \"\"\"\n",
    "    Generates text using the LRU_LLM language model.\n",
    "    \n",
    "    Args:\n",
    "        model: LRU_LLM model instance.\n",
    "        seed: Initial seed text to start generation.\n",
    "        max_length: Maximum length of the generated text.\n",
    "    \n",
    "    Returns:\n",
    "        Generated text as a string.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize input text    \n",
    "    generated_text = prompt\n",
    "    tokens = tokenizer.tokenize(generated_text, max_length=gen_length)\n",
    "    padding_free = tokenizer.tokenize(generated_text) # generate without padding\n",
    "    tokens_length = len(padding_free) - 1 # + 1 for BOS, -1 for EOS\n",
    "\n",
    "    # get next token prediction for all tokens, including padding.\n",
    "    # Set the first </s> in context to the predicted next token, then iterate.\n",
    "    for i in range(tokens_length, gen_length):\n",
    "        to_ = i\n",
    "        from_ = i-1\n",
    "        #tokens[i] = predictions(tokens, model, params)[i-1].item()\n",
    "        predicted_token = predictions(tokens, model, params, wanted_index=from_, temp=temp).item()\n",
    "        tokens[to_] = predicted_token\n",
    "\n",
    "    generated_text = tokenizer.detokenize(tokens)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s><s>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model=lru_LLM, params=lru_LLM_params, prompt='', gen_length=32, temp=0.2))\n",
    "\n",
    "# with a small amount of layers, considering weight tying and skip connections and how LRU is initialized near identity, an initialized LRU-LLM will repeat the input.\n",
    "\n",
    "# generation is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "0 loss: 10.8125\n",
      "================= GENERATED =================\n",
      "<s><s><s><s><s><s><s>goinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoinggoing陳 \\({\\ \\({\\ \\({\\ \\({\\jpgjpgjpgjpgjpgjpgjpg.~.~.~.~czy José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José José\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "500 loss: 4.89425\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "What is the following to get <p>I'm trying to write a way to get a file file? <p>I'm trying to get the best way to do this:</p>\n",
      "\n",
      "<pre><code>import time\n",
      "\n",
      "\n",
      "\n",
      "    def __init__(self, 1\n",
      "\n",
      "\n",
      "&gt;&gt;&gt;\n",
      "\n",
      "    self. = 2.0.00000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1000 loss: 4.00921875\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python 2666625552555555667685555566657500000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "1500 loss: 3.81225\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to a large Python using python <p>I have a function that can't seem to find the code of the program that's what is the time, and I'm not sure how to the current code to a file. I have a function that I'm trying to use the user's using the data, but I'm using a file that you can get the user in the string, but it's not a python.  I'm using the function to use the same page. I have a file in python. I'm using python <code>foo.py</code> in <code>f_file</code> in <code>_name</code>?</p>\n",
      "\n",
      "<p>So, I have to make the following line:</p>\n",
      "\n",
      "<pre><code>class A(models.Model):\n",
      "    def __init__(self, value, value, [2,1,1,3,3,3,5,3,5,5,3,5,5,5,5,3,0,5,0,0,0,0,0,0,0,0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2000 loss: 3.666875\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python: How to the same value in python? <p>I have a Python program that's the best way to do this?</p>\n",
      "\n",
      "\n",
      "-<{ANSWER}>-\n",
      "\n",
      "<p>If you're using Python's <code>__init__.py</code> in Python <code>test</code> and <code>test</code> is that the <code>sys.getcode</code> is not in the <code>test</code> as a problem, I can't find the following code:</p>\n",
      "\n",
      "<pre><code>from django.contrib.get_table = models.CharField(max_length=200)\n",
      "    def __init__(self):\n",
      "        self.var = \"r\", \"2\", \"2\", \"D\", \"hello\"\n",
      "\n",
      "#\n",
      "</code></pre>\n",
      "\n",
      "<p>Is there a way to do this?</p>\n",
      "\n",
      "\n",
      "-<{ANSWER}>-\n",
      "\n",
      "<p>The <a href=\"http://docs.python.org/library/urllib2.html#from Python.html\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2500 loss: 3.539375\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to create a list of 2.6. <p>I have a python script that can be able to make the <code>dict</code> and <code>__init__</code> is the code that I can't seem to get the file. The first line is that it's the current method, and the name of the form.  I'm trying to do this:</p>\n",
      "\n",
      "<pre><code>class MyModel(models.Model):\n",
      "    def __init__(self, self.request.read()\n",
      "\n",
      "    def __init__(self, self.list.append(User)\n",
      "    self.objects.filter(key=None)\n",
      "    self.login.id.findall()\n",
      "</code></pre>\n",
      "\n",
      "<p>I'm using the following error:</p>\n",
      "\n",
      "<pre><code>class MyClass(models.Model):\n",
      "    if not found\n",
      "</code></pre>\n",
      "\n",
      "<p>How can I do this?</p>\n",
      "\n",
      "\n",
      "-<{ANSWER}>-\n",
      "\n",
      "<p>If you're looking for the same thing to use the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3000 loss: 3.4814375\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "python setup.py file <p>I am trying to get a list of my list of a python script. I'm not sure if I have a problem with a script, but I am not sure how to do this?</p>\n",
      "\n",
      "<p>I'm not sure how to do this, but I'm not sure if it is not possible to do this?</p>\n",
      "\n",
      "<p>I'm trying to do this in a Python script, I have to make a new file, and it's not in the code. I'm trying to write a file that I can't find a more more than a new window, and I'm not sure how to do this?</p>\n",
      "\n",
      "<p>I am using a <code>is</code> method</p>\n",
      "\n",
      "<p>I am using <code>my_list</code> and <code>import sys.path</code> is <code>int</code> in <code>my_to_dir</code> and the following error:</p>\n",
      "\n",
      "<pre><code>import sys\n",
      "import sys\n",
      "\n",
      "def foo(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3500 loss: 3.43890625\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python: How to a python module? <p>I'm trying to make a python script that I'm using Python and the <code>__getattr__</code> in <code>if</code> and <code>print</code> is <code>list</code> in <code>f.py</code> and <code>str</code> to <code>n</code> is the same thing to be the same time, and I'm using the code in the case of the second.  This is the correct helped, but the other code is there any way to use the same page?</p>\n",
      "\n",
      "<p>I'm trying to get a program to make a newbie to the same file. The problem is that I'm having to use the 2.6.4 and 2.6.4.5.2.6.4.4.1.2.1.2.3.1.2.1.2.1.2.1.2.2.1.2.2.1.2.2.2.4.2.2.6.4.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4000 loss: 3.39090625\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python, if a module <p>I'm trying to get a Python script that's a lot of dictionaries that is not the following:</p>\n",
      "\n",
      "<pre><code>import urllib\n",
      "\n",
      "def main():\n",
      "    print \"abc\"\n",
      "\n",
      "    def __init__(self):\n",
      "        self.db.get('data')\n",
      "        self.list = self.query.get(self.db)\n",
      "\n",
      "    def __init__(self, parent, self):\n",
      "        self.f = 's'\n",
      "\n",
      "        if '1':\n",
      "        print 'S',\n",
      "\n",
      "    def __init__(self):\n",
      "        self.root = self.request.get(self.default)\n",
      "\n",
      "    def __init__(self, parent,\n",
      "        self.subplot(0,\n",
      "        self.list = models.CharField(max_length=100)\n",
      "    return self.text = db.objects.filter(url)\n",
      "\n",
      "class CreateForm(self.path):\n",
      "    # This is not found\n",
      "\n",
      "    def __init__(self, name):\n",
      "        self.request.filter(self.is_name)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4500 loss: 3.36665625\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python - Django: I'm not sure how to do this? <p>I'm trying to write a function to get an error:</p>\n",
      "\n",
      "<pre><code>#!/usr/bin/env python\n",
      "import sys\n",
      "import urllib\n",
      "import urllib\n",
      "import urllib\n",
      "import matplotlib\n",
      "import sys\n",
      "import urllib\n",
      "import matplotlib\n",
      "import matplotlib\n",
      "import re\n",
      "import sys\n",
      "import sys\n",
      "import sys\n",
      "import sys\n",
      "import sys\n",
      "import time\n",
      "\n",
      "\n",
      "import sys\n",
      "\n",
      "import sys\n",
      "\n",
      "class MainModel(models.Model):\n",
      "    name = models.CharField(max_length=200)\n",
      "    # convert the value\n",
      "    return\n",
      "    def __init__(self, parent, name):\n",
      "        self.c = self.c.get_value()\n",
      "\n",
      "    def __init__(self, parent, id):\n",
      "        self.name = self.cursor()\n",
      "\n",
      "        self.name = \n",
      "        self.get_get_list()\n",
      "\n",
      "    def __init__(self, parent, value):\n",
      "        self.set_field_f_response.query(sys.argv[0]\n",
      "</\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5000 loss: 3.309125\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to parse a function in python <p>I have a python script that I have a list of a dictionary that I'm using the following:</p>\n",
      "\n",
      "<pre><code>class Base(object):\n",
      "    def __init__(self, parent, v):\n",
      "        self.cursor = self.query.get('a')\n",
      "\n",
      "    def __init__(self, x, y):\n",
      "        self.a = \n",
      "        self.value = \"import *\n",
      "\n",
      "        def __init__(self, self,None):\n",
      "        self.data = models.CharField(max_length=200)\n",
      "    while True:\n",
      "        print \"Test\": \"name\": \"\")\n",
      "    print \"\n",
      "    #    # \"self.response.encode(\"1\")\n",
      "        print \"my_file\"\n",
      "\n",
      "    def __init__(self, x, y,self):\n",
      "        self.start = 'list'\n",
      "        self.get_func(self, value)\n",
      "        self.set_out = \"\n",
      "        self.write(f)\n",
      "\n",
      "        self.s = []\n",
      "        self.name = self.list_type\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5500 loss: 3.28671875\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python: This is not possible to make the file <p>I have a Python script that's a file, and then you'll need to be a file. The <code>re.sub</code> to have the following code:</p>\n",
      "\n",
      "<pre><code>#!/usr/bin/env python\n",
      "\n",
      "import sys\n",
      "import os\n",
      "import sys\n",
      "import os\n",
      "import sys\n",
      "import sys\n",
      "import os\n",
      "import sys\n",
      "import os\n",
      "import os\n",
      "import os\n",
      "import sys\n",
      "import sys\n",
      "import os\n",
      "import sys\n",
      "import os\n",
      "import sys\n",
      "import os\n",
      "import sys\n",
      "import sys\n",
      "import sys\n",
      "import sys\n",
      "import sys\n",
      "\n",
      "def main():\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        return self.class_x(self):\n",
      "        self.data = []\n",
      "\n",
      "        self.user = '1.2\n",
      "        self.f = open('myfile.txt')\n",
      "\n",
      "    print \"hello\"\n",
      "\n",
      "\n",
      "class test(models.Model):\n",
      "    name = models.CharField(max_length=100)\n",
      "    except KeyError:\n",
      "        return False\n",
      "\n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6000 loss: 3.31028125\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python: I get the list of the value in a string? <p>I have a list of lists and the user's <code>s = 1</code> is an example of the following:</p>\n",
      "\n",
      "<pre><code>import numpy as np\n",
      "\n",
      "class C(models.Model):\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        self.a = \n",
      "        self.data = None\n",
      "\n",
      "        self.get_get('self.name)\n",
      "        self.add_content = 'foo.txt', 'c': 'c': '1', 'c', 'r', 'f', 'w', 'd', 'w', '', 'name', 'bar', 'id', 'c', 'w', '1', 'B', 'C', '2', 'w', 'w', 'a', 'http', '1', 'b', '1', '4', '2', '3', 'B', '', '0', '0', '0', 'x', '2', 'r', 'r', '3', 'b', 'b', '', 'w\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6500 loss: 3.3075625\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python: How to use the list <p>I have a python script that will allow me to get a <code>x</code> or <code>list</code> method, <code>for</code> as <code>d</code> to the <code>from_items</code> and <code>datetime.datetime.now()</code> that it doesn't work.  I'm not sure how to do this?</p>\n",
      "\n",
      "\n",
      "-<{ANSWER}>-\n",
      "\n",
      "<p>You can use <a href=\"http://docs.python.org/library/functions.html#function-urllib-urllib.html\" rel=\"nofollow\">http://docs.python.org/library/stdtypes.html#str.html\" rel=\"nofollow\">http://docs.python.org/library/subprocess.html\" rel=\"nofollow\">http://www.webapp.com/products/2010/01/python-python-2.6.1-8.1-2.1.1.1.1.1.1.1.1.1.1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7000 loss: 3.28384375\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}> Image\n",
      "\n",
      "How to use the Python interpreter to do with the following code? <p>I'm trying to use a Python script that will be a few of the web service to use <a href=\"http://docs.python.org/library/stdtypes.html#str.html\" rel=\"nofollow\">http://www.scipy.org/doc/scipy/reference/scipy.org/doc/html/datastore/\">http://docs.python.org/library/stdtypes.html#subprocess.Popen\" rel=\"nofollow\">http://docs.python.org/library/subprocess.html\" rel=\"nofollow\">http://docs.python.org/library/re.html\" rel=\"nofollow\">http://www.wxpython.org/docs/python/reference/library/os.html#os.path.join\"><code>urllib2.urlopen('http://www.google.com/search/trunk/docs/python/</a></p>\n",
      "\n",
      "<p>I'm using the following error:</p>\n",
      "\n",
      "<pre><code>import sys\n",
      "\n",
      "def get_instance(\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7500 loss: 3.27353125\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python/3.py -c-c: utf-8 -*-\n",
      "import re\n",
      "import re\n",
      "import os\n",
      "import os\n",
      "import sys\n",
      "import sys\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import sys\n",
      "\n",
      "import os\n",
      "\n",
      "class Parent(models.Model):\n",
      "    def __init__(self, parent, id):\n",
      "        self.args = \n",
      "        self.set = db.StringProperty()\n",
      "\n",
      "    def get_self(self):\n",
      "        return self.fork()\n",
      "        self.m = 0\n",
      "        self.response = 0\n",
      "        self.get_path = self.socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
      "    self.name = \"id\"\n",
      "    print \"test\"\n",
      "\n",
      "    print \"s\" %s\" % \"\n",
      "    \"file\"\n",
      "    print \"test\"\n",
      "\n",
      "def f(self):\n",
      "    return True\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        self.method = self.get_data = []\n",
      "        self.set = []\n",
      "        self.c = 'a'\n",
      "       \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8000 loss: 3.289\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python - PyDev in Python <p>I am trying to create a customize the server and the python script to run it.  The problem is that I can't find the program, but it doesn't work. I am using the <code>re.find</code> and <code>my_input</code> is not <code>__getattr__</code>?</p>\n",
      "\n",
      "<p>I'm using the <code>s</code> method to <code>x</code> to <code>b</code> and <code>code</code> and <code>x</code> and <code>__</code> is <code>A</code> and <code>my_name</code> is <code>1</code> (the <code>foo</code> as <code>1</code> is a <code>1</code> and <code>y</code> to <code>dict</code> in <code>file</code> and <code>__init__</code> to <code>e</code> is a <code>for</code\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8500 loss: 3.27184375\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to make a list of text in a dictionary? <p>I have a list of strings and <code>gobject</code> to get the <code>pyplot</code> function.  </p>\n",
      "\n",
      "<pre><code>from django.contrib.auth.models import User\n",
      "\n",
      "class Post(models.Model):\n",
      "    name = models.CharField(max_length=200)\n",
      "    if not in &lt;module&gt;\n",
      "    from django.py\n",
      "    def __init__(self, parent, self.id, self.args, **kwargs)\n",
      "        self.add_to_response(self.id)\n",
      "        self.set_name(self.name, 'w')\n",
      "\n",
      "    return render_to_response('test.html')\n",
      "</code></pre>\n",
      "\n",
      "<p>I want to be able to get the following error:</p>\n",
      "\n",
      "<pre><code>def print(self, name):\n",
      "    print \"Hello World\"\n",
      "</code></pre>\n",
      "\n",
      "<p>And the same thing, but I'm not sure how to do this?</p>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9000 loss: 3.27053125\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "Python: This function to have an attribute in a file <p>I have a string that contains a string in a file, but it's not a <code>file</code> function, but it seems to be a <code>dict</code> in the <code>s</code> and <code>s</code> and <code>f</code> in <code>my_list</code> is the following error:</p>\n",
      "\n",
      "<pre><code>def my_list(1, 2, 3, 4, 2, 1, 2, 3, 4, 5, 5, 5, 1, 2, 1, 1, 2, 1, 2, 1, 2, 3, 4, 5, 4, 5, 1, 1, 2, 1, 1, 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, 3, 4, 5, 2, 1,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9500 loss: 3.27453125\n",
      "================= GENERATED =================\n",
      "<s> -<{QUESTION}>-\n",
      "\n",
      "How to get the key value in a file in python? <p>I have a list of lists of a string, and I want to make a user input, I get the following error:</p>\n",
      "\n",
      "<pre><code>#!/usr/bin/python\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import sys\n",
      "import os\n",
      "import os\n",
      "import os\n",
      "import os\n",
      "import os\n",
      "import os\n",
      "import sys\n",
      "import sys\n",
      "import os\n",
      "import os\n",
      "import sys\n",
      "import sys\n",
      "import sys\n",
      "import time\n",
      "\n",
      "\n",
      "import os\n",
      "import sys\n",
      "import time\n",
      "\n",
      "import time\n",
      "\n",
      "import sys\n",
      "\n",
      "def re(self, c):\n",
      "    self.a = 0\n",
      "\n",
      "    def __init__(self, *args, **kwargs):\n",
      "        self.name = name\n",
      "        self.get_name = self.socket.get_key_id = db.StringProperty()\n",
      "    if len(name) == 0:\n",
      "        print 'b'\n",
      "</code></pre>\n",
      "\n",
      "<p>and then I get the error:</p>\n",
      "\n",
      "<pre><code>from Tkinter import *\n",
      "\n",
      "\n",
      "def add\n"
     ]
    }
   ],
   "source": [
    "from flax.training import train_state\n",
    "\n",
    "data_loader = SimpleDataLoader(dataset_samples, batch_size=batch_size, context_length=ctx_size, tokenizer=tokenizer)\n",
    "\n",
    "# During training, make sample generations. We can add a prompt to this.\n",
    "prompt = '-<\\{QUESTION\\}>-\\n\\nPython 3.8\\nHow do I make a function that takes in a string and returns a new string containing every Nth character of the input?'\n",
    "prompt = ''\n",
    "# nucleus sampling temperature to use for generation during training\n",
    "temp = 0.3\n",
    "\n",
    "# Model and optimizer\n",
    "schedule = optax.cosine_decay_schedule(init_value=lr, decay_steps=iterations)\n",
    "optimizer = optax.adam(learning_rate=schedule)\n",
    "state = train_state.TrainState.create(apply_fn=lru_LLM.apply, params=lru_LLM_params['params'], tx=optimizer)\n",
    "\n",
    "gen_frequency = 500\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "accs = []\n",
    "for epoch in range(1):\n",
    "\n",
    "    for step in range(iterations):\n",
    "\n",
    "        # get batch data\n",
    "        xs, ys = data_loader.get_batch(step * batch_size)\n",
    "\n",
    "        def loss_func(params):\n",
    "            #get logits\n",
    "            logits = state.apply_fn({'params': params}, xs)\n",
    "            # get loss\n",
    "            loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=ys).mean()\n",
    "            return loss, logits\n",
    "                \n",
    "        gradient_fn = jax.value_and_grad(loss_func, has_aux=True)\n",
    "        (loss, logits), grads = gradient_fn(state.params)\n",
    "\n",
    "        acc = (logits.argmax(axis=-1) == ys).mean()\n",
    "        losses.append(loss.item())\n",
    "        accs.append(acc.item())\n",
    "        # print(loss, acc)\n",
    "\n",
    "        # step parameters\n",
    "        state = state.apply_gradients(grads=grads)\n",
    "\n",
    "\n",
    "        if step%gen_frequency == 0:\n",
    "            print(\"\\n\\n\\n\")\n",
    "            if step>0:\n",
    "                print(step, 'loss:', np.asarray(losses[-gen_frequency:]).mean())\n",
    "            else:\n",
    "                print(step, 'loss:', losses[-1])\n",
    "            \n",
    "            # mostly for debugging - feeds the LLM an input sample, gets the top prediction for each token. \n",
    "            # NOTE: Since LRU is initizliaed near identity, if using weight tying and small number of layers, \n",
    "            # at initialization we will simply see it repeat the input, so it will look like its outputting perfectly coherent text, correctly predicting every token,\n",
    "            # but its actually shifted by 1 compared to ground truth - its just outputting the most recent token :)\n",
    "            #\n",
    "            # print(\" ======= DECODED:\")\n",
    "            # print(tokenizer.detokenize(logits[0,:,0:vocab_size].argmax(axis=-1)[:].tolist()))\n",
    "            \n",
    "            print(\"================= GENERATED =================\")\n",
    "            print(generate(lru_LLM, params={'params':state.params}, prompt=prompt, gen_length=ctx_size, temp=temp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f558608e430>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
